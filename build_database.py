#!/usr/bin/env python3
"""
Unified Database Builder for Kanji Reader
Creates a complete FTS5-enabled database ready for Android app deployment

This script combines all the successful database building steps:
1. Creates main dictionary_entries table with FTS5 search
2. Creates kanji_entries table for KanjiDic data
3. Populates with JMdict and KanjiDic data
4. Tokenizes Japanese text properly for search
5. Creates all necessary indexes
6. Saves to app/src/main/assets/databases/
"""

import sqlite3
import json
import os
import sys
import shutil
from typing import List, Dict, Tuple, Optional
from datetime import datetime

# Try to import MeCab if available
try:
    import MeCab
    MECAB_AVAILABLE = True
except ImportError:
    MECAB_AVAILABLE = False
    print("‚ö†Ô∏è  MeCab not available - will use basic tokenization")

def get_radical_name(radical_number: int) -> str:
    """
    Map Kangxi radical numbers to their Japanese names.
    This is a subset of the most common radicals.
    """
    radical_names = {
        1: "‰∏Ä", 2: "‰∏®", 3: "‰∏∂", 4: "‰∏ø", 5: "‰πô", 6: "‰∫Ö", 7: "‰∫å", 8: "‰∫†", 9: "‰∫∫", 10: "ÂÑø",
        11: "ÂÖ•", 12: "ÂÖ´", 13: "ÂÜÇ", 14: "ÂÜñ", 15: "ÂÜ´", 16: "Âá†", 17: "Âáµ", 18: "ÂàÄ", 19: "Âäõ", 20: "Âãπ",
        21: "Âåï", 22: "Âåö", 23: "Âå∏", 24: "ÂçÅ", 25: "Âçú", 26: "Âç©", 27: "ÂéÇ", 28: "Âé∂", 29: "Âèà", 30: "Âè£",
        31: "Âõó", 32: "Âúü", 33: "Â£´", 34: "Â§Ç", 35: "Â§ä", 36: "Â§ï", 37: "Â§ß", 38: "Â•≥", 39: "Â≠ê", 40: "ÂÆÄ",
        41: "ÂØ∏", 42: "Â∞è", 43: "Â∞¢", 44: "Â∞∏", 45: "Â±Æ", 46: "Â±±", 47: "Â∑ù", 48: "Â∑•", 49: "Â∑±", 50: "Â∑æ",
        51: "Âπ≤", 52: "Âπ∫", 53: "Âπø", 54: "Âª¥", 55: "Âªæ", 56: "Âºã", 57: "Âºì", 58: "ÂΩê", 59: "ÂΩ°", 60: "ÂΩ≥",
        61: "ÂøÉ", 62: "Êàà", 63: "Êà∏", 64: "Êâã", 65: "ÊîØ", 66: "Êî¥", 67: "Êñá", 68: "Êñó", 69: "Êñ§", 70: "Êñπ",
        71: "Êó†", 72: "Êó•", 73: "Êõ∞", 74: "Êúà", 75: "Êú®", 76: "Ê¨†", 77: "Ê≠¢", 78: "Ê≠π", 79: "ÊÆ≥", 80: "ÊØã",
        81: "ÊØî", 82: "ÊØõ", 83: "Ê∞è", 84: "Ê∞î", 85: "Ê∞¥", 86: "ÁÅ´", 87: "Áà™", 88: "Áà∂", 89: "Áàª", 90: "Áàø",
        91: "Áâá", 92: "Áâô", 93: "Áâõ", 94: "Áä¨", 95: "ÁéÑ", 96: "Áéâ", 97: "Áìú", 98: "Áì¶", 99: "Áîò", 100: "Áîü",
        101: "Áî®", 102: "Áî∞", 103: "Áñã", 104: "Áñí", 105: "Áô∂", 106: "ÁôΩ", 107: "ÁöÆ", 108: "Áöø", 109: "ÁõÆ", 110: "Áüõ",
        111: "Áü¢", 112: "Áü≥", 113: "Á§∫", 114: "Á¶∏", 115: "Á¶æ", 116: "Á©¥", 117: "Á´ã", 118: "Á´π", 119: "Á±≥", 120: "Á≥∏",
        121: "Áº∂", 122: "ÁΩë", 123: "Áæä", 124: "ÁæΩ", 125: "ËÄÅ", 126: "ËÄå", 127: "ËÄí", 128: "ËÄ≥", 129: "ËÅø", 130: "ËÇâ",
        131: "Ëá£", 132: "Ëá™", 133: "Ëá≥", 134: "Ëáº", 135: "Ëàå", 136: "Ëàõ", 137: "Ëàü", 138: "ËâÆ", 139: "Ëâ≤", 140: "Ëâ∏",
        141: "Ëôç", 142: "Ëô´", 143: "Ë°Ä", 144: "Ë°å", 145: "Ë°£", 146: "Ë•æ", 147: "Ë¶ã", 148: "Ëßí", 149: "Ë®Ä", 150: "Ë∞∑",
        151: "Ë±Ü", 152: "Ë±ï", 153: "Ë±∏", 154: "Ë≤ù", 155: "Ëµ§", 156: "Ëµ∞", 157: "Ë∂≥", 158: "Ë∫´", 159: "Ëªä", 160: "Ëæõ",
        161: "Ëæ∞", 162: "Ëæµ", 163: "ÈÇë", 164: "ÈÖâ", 165: "ÈáÜ", 166: "Èáå", 167: "Èáë", 168: "Èï∑", 169: "ÈñÄ", 170: "Èòú",
        171: "Èö∂", 172: "Èöπ", 173: "Èõ®", 174: "Èùí", 175: "Èùû", 176: "Èù¢", 177: "Èù©", 178: "Èüã", 179: "Èü≠", 180: "Èü≥",
        181: "È†Å", 182: "È¢®", 183: "È£õ", 184: "È£ü", 185: "È¶ñ", 186: "È¶ô", 187: "È¶¨", 188: "È™®", 189: "È´ò", 190: "È´ü",
        191: "È¨•", 192: "È¨Ø", 193: "È¨≤", 194: "È¨º", 195: "È≠ö", 196: "È≥•", 197: "Èπµ", 198: "Èπø", 199: "È∫¶", 200: "È∫ª",
        201: "ÈªÑ", 202: "Èªç", 203: "Èªí", 204: "Èªπ", 205: "ÈªΩ", 206: "Èºé", 207: "Èºì", 208: "Èº†", 209: "Èºª", 210: "ÈΩä",
        211: "ÈΩí", 212: "Á´ú", 213: "‰∫Ä", 214: "Èæ†"
    }
    return radical_names.get(radical_number, "")

class DatabaseBuilder:
    def __init__(self, output_path: str = "app/src/main/assets/databases/jmdict_fts5.db"):
        self.output_path = output_path
        self.mecab_tokenizer = None
        self.frequency_data = {}
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Load frequency data
        self.load_frequency_data()
        
        # Initialize MeCab if available
        if MECAB_AVAILABLE:
            try:
                # IMPORTANT: For Windows, you might need to specify the path to mecabrc
                # For WSL/Linux, MeCab should usually be found if installed via package manager
                if sys.platform == "win32":
                    # Adjust this path if your MeCab installation is elsewhere
                    mecabrc_path = os.path.join(os.getenv('MECAB_PATH', r"C:\Program Files\MeCab"), "etc", "mecabrc")
                    if os.path.exists(mecabrc_path):
                        os.environ["MECABRC"] = mecabrc_path
                    else:
                        print(f"‚ö†Ô∏è  MECABRC not found at {mecabrc_path}. MeCab might not initialize correctly.")
                self.mecab_tokenizer = MeCab.Tagger()
                print("‚úÖ MeCab tokenizer initialized")
            except Exception as e:
                print(f"‚ö†Ô∏è  MeCab initialization failed: {e}")
                print("   Falling back to basic tokenization")

    def load_frequency_data(self) -> None:
        """Load frequency data from CSV file"""
        import csv
        frequency_file = "app/src/main/assets/frequency_data.csv"

        if not os.path.exists(frequency_file):
            print(f"‚ö†Ô∏è  Frequency file not found: {frequency_file}")
            return

        try:
            with open(frequency_file, 'r', encoding='utf-8-sig') as f:
                # Skip BOM if present
                reader = csv.reader(f)
                next(reader)  # Skip header

                count = 0
                duplicates_found = 0
                skipped_corrupted = 0

                for row in reader:
                    if len(row) >= 4:
                        word = row[0].strip()
                        pos = row[1].strip() if len(row) > 1 else ""
                        reading = row[2].strip() if len(row) > 2 else ""
                        frequency_str = row[3].strip()

                        # Skip genuinely corrupted data (empty words or missing frequencies)
                        if not word or not frequency_str or frequency_str.strip() == '':
                            skipped_corrupted += 1
                            continue

                        # Parse frequency: remove commas and spaces, convert to int
                        try:
                            frequency = int(frequency_str.replace(',', '').replace(' ', ''))

                            # Create a unique key for true duplicates (same word, same POS, same reading)
                            unique_key = f"{word}|{pos}|{reading}"

                            # Keep the highest frequency for true duplicates
                            if unique_key in self.frequency_data:
                                existing_freq = self.frequency_data[unique_key]
                                if frequency > existing_freq:
                                    print(f"   üîÑ Duplicate: '{word}' ({pos}|{reading}) - replacing freq {existing_freq:,} with {frequency:,}")
                                    self.frequency_data[unique_key] = frequency
                                    # Also update the simple word key for lookup
                                    if word not in self.frequency_data or frequency > self.frequency_data[word]:
                                        self.frequency_data[word] = frequency
                                else:
                                    print(f"   ‚ùå Duplicate: '{word}' ({pos}|{reading}) - keeping existing freq {existing_freq:,}, discarding {frequency:,}")
                                duplicates_found += 1
                            else:
                                self.frequency_data[unique_key] = frequency
                                # Also store by simple word key, keeping highest frequency
                                if word not in self.frequency_data or frequency > self.frequency_data[word]:
                                    self.frequency_data[word] = frequency
                                count += 1
                        except ValueError:
                            skipped_corrupted += 1
                            continue

                print(f"‚úÖ Loaded {count} frequency entries from CSV")
                if duplicates_found > 0:
                    print(f"‚ö†Ô∏è  Found {duplicates_found} true duplicate entries - kept highest frequencies")
                if skipped_corrupted > 0:
                    print(f"‚ö†Ô∏è  Skipped {skipped_corrupted} corrupted/empty entries")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error loading frequency data: {e}")

    def create_database_schema(self, conn: sqlite3.Connection) -> None:
        """Create all necessary tables and indexes"""
        cursor = conn.cursor()

        print("üìã Creating database schema...")

        # Main dictionary entries table
        # Schema without jmnedict and form_is_common columns
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS dictionary_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                kanji TEXT,
                reading TEXT NOT NULL,
                meanings TEXT NOT NULL,
                parts_of_speech TEXT,
                is_common INTEGER DEFAULT 0,
                frequency INTEGER DEFAULT 0,
                tokenized_kanji TEXT,
                tokenized_reading TEXT,
                UNIQUE(kanji, reading)
            )
        """)

        # Create indexes for performance
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_reading ON dictionary_entries(reading)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_kanji ON dictionary_entries(kanji)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_common ON dictionary_entries(is_common)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_frequency ON dictionary_entries(frequency)")

        # Create FTS5 virtual tables
        print("üìä Creating FTS5 virtual tables...")

        # Japanese text search table (entries_fts5)
        # FTS table without jmnedict and form_is_common columns
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS entries_fts5 USING fts5(
                kanji,
                reading,
                meanings,
                tokenized_kanji,
                tokenized_reading,
                parts_of_speech,
                is_common UNINDEXED,
                frequency UNINDEXED,
                content='dictionary_entries',
                content_rowid='id'
            )
        """)

        # English search table
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS english_fts USING fts5(
                entry_id UNINDEXED,
                meanings,
                parts_of_speech,
                tokenize='unicode61',
                prefix='1 2 3'
            )
        """)

        # Japanese substring search table with individual n-gram tokens
        # This enables fast substring matching (e.g., find „ÇÇ„Å¶„ÅÇ„Åù„Å∂ when searching „ÅÇ„Åù„Å∂)
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS japanese_substring_fts USING fts5(
                entry_id UNINDEXED,
                ngram
            )
        """)

        # Tag definitions and word tags
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS tag_definitions (
                tag TEXT PRIMARY KEY,
                description TEXT
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS word_tags (
                entry_id INTEGER,
                tag TEXT,
                FOREIGN KEY(entry_id) REFERENCES dictionary_entries(id) ON DELETE CASCADE,
                FOREIGN KEY(tag) REFERENCES tag_definitions(tag) ON DELETE CASCADE
            )
        """)

        # Create indexes for tag tables
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_word_tags_entry ON word_tags(entry_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_word_tags_tag ON word_tags(tag)")

        # Pitch accent table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS pitch_accents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                kanji_form TEXT NOT NULL,
                reading TEXT NOT NULL,
                accent_pattern TEXT NOT NULL,
                UNIQUE(kanji_form, reading)
            )
        """)
        
        # Word variants table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS word_variants (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                jmdict_id TEXT NOT NULL,
                primary_kanji TEXT NOT NULL,
                variant_kanji TEXT NOT NULL,
                reading TEXT NOT NULL,
                meaning TEXT,
                UNIQUE(primary_kanji, variant_kanji, reading)
            )
        """)

        # Create indexes for pitch accent table
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_pitch_kanji ON pitch_accents(kanji_form)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_pitch_reading ON pitch_accents(reading)")
        
        # Create indexes for word variants table
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_wv_primary ON word_variants(primary_kanji)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_wv_variant ON word_variants(variant_kanji)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_wv_jmdict ON word_variants(jmdict_id)")

        print("‚úÖ Dictionary database schema created")

        # Create KanjiDic tables in the same database
        self.create_kanjidic_schema(conn)
        conn.commit()

    def populate_tag_definitions(self, conn: sqlite3.Connection) -> None:
        """Populate tag definitions table with JMdict POS tags"""
        cursor = conn.cursor()

        print("üè∑Ô∏è  Populating tag definitions...")

        # Complete tag definitions from jmdict.json
        tag_definitions = {
            # Basic parts of speech
            'n': 'noun (common) (futsuumeishi)',
            'n-adv': 'adverbial noun (fukushitekimeishi)', 
            'n-t': 'noun (temporal) (jisoumeishi)',
            'n-pr': 'proper noun',
            'n-pref': 'noun, used as a prefix',
            'n-suf': 'noun, used as a suffix',
            'adj-i': 'adjective (keiyoushi)',
            'adj-na': 'adjectival nouns or quasi-adjectives (keiyodoshi)',
            'adj-no': 'nouns which may take the genitive case particle \'no\'',
            'adj-pn': 'pre-noun adjectival (rentaishi)',
            'adj-t': '\'taru\' adjective',
            'adj-f': 'noun or verb acting prenominally',
            'adj-ix': 'adjective (keiyoushi) - yoi/ii class',
            'adj-ku': '\'ku\' adjective (archaic)',
            'adj-shiku': '\'shiku\' adjective (archaic)',
            'adj-nari': 'archaic/formal form of na-adjective',
            'adj-kari': '\'kari\' adjective (archaic)',
            'adv': 'adverb (fukushi)',
            'adv-to': 'adverb taking the \'to\' particle',
            'aux': 'auxiliary',
            'aux-v': 'auxiliary verb',
            'aux-adj': 'auxiliary adjective',
            'conj': 'conjunction',
            'cop': 'copula',
            'ctr': 'counter',
            'exp': 'expressions (phrases, clauses, etc.)',
            'id': 'idiomatic expression',
            'int': 'interjection (kandoushi)',
            'num': 'numeric',
            'pn': 'pronoun',
            'prt': 'particle',
            'pref': 'prefix',
            'suf': 'suffix',

            # Verbs - Regular
            'v1': 'Ichidan verb',
            'v1-s': 'Ichidan verb - kureru special class',
            'v5b': 'Godan verb with \'bu\' ending',
            'v5g': 'Godan verb with \'gu\' ending',
            'v5k': 'Godan verb with \'ku\' ending',
            'v5k-s': 'Godan verb - Iku/Yuku special class',
            'v5m': 'Godan verb with \'mu\' ending',
            'v5n': 'Godan verb with \'nu\' ending',
            'v5r': 'Godan verb with \'ru\' ending',
            'v5r-i': 'Godan verb with \'ru\' ending (irregular verb)',
            'v5s': 'Godan verb with \'su\' ending',
            'v5t': 'Godan verb with \'tsu\' ending',
            'v5u': 'Godan verb with \'u\' ending',
            'v5u-s': 'Godan verb with \'u\' ending (special class)',
            'v5uru': 'Godan verb - Uru old class verb (old form of Eru)',
            'v5aru': 'Godan verb - -aru special class',
            'vk': 'Kuru verb - special class',
            'vn': 'irregular nu verb',
            'vr': 'irregular ru verb, plain form ends with -ri',
            'vs': 'noun or participle which takes the aux. verb suru',
            'vs-c': 'su verb - precursor to the modern suru',
            'vs-s': 'suru verb - special class',
            'vs-i': 'suru verb - included',
            'vz': 'Ichidan verb - zuru verb (alternative form of -jiru verbs)',
            'vt': 'transitive verb',
            'vi': 'intransitive verb',
            'v-unspec': 'verb unspecified',

            # Archaic verbs
            'v2a-s': 'Nidan verb with \'u\' ending (archaic)',
            'v2b-k': 'Nidan verb (upper class) with \'bu\' ending (archaic)',
            'v2b-s': 'Nidan verb (lower class) with \'bu\' ending (archaic)',
            'v2d-k': 'Nidan verb (upper class) with \'dzu\' ending (archaic)',
            'v2d-s': 'Nidan verb (lower class) with \'dzu\' ending (archaic)',
            'v2g-k': 'Nidan verb (upper class) with \'gu\' ending (archaic)',
            'v2g-s': 'Nidan verb (lower class) with \'gu\' ending (archaic)',
            'v2h-k': 'Nidan verb (upper class) with \'hu/fu\' ending (archaic)',
            'v2h-s': 'Nidan verb (lower class) with \'hu/fu\' ending (archaic)',
            'v2k-k': 'Nidan verb (upper class) with \'ku\' ending (archaic)',
            'v2k-s': 'Nidan verb (lower class) with \'ku\' ending (archaic)',
            'v2m-k': 'Nidan verb (upper class) with \'mu\' ending (archaic)',
            'v2m-s': 'Nidan verb (lower class) with \'mu\' ending (archaic)',
            'v2n-s': 'Nidan verb (lower class) with \'nu\' ending (archaic)',
            'v2r-k': 'Nidan verb (upper class) with \'ru\' ending (archaic)',
            'v2r-s': 'Nidan verb (lower class) with \'ru\' ending (archaic)',
            'v2s-s': 'Nidan verb (lower class) with \'su\' ending (archaic)',
            'v2t-k': 'Nidan verb (upper class) with \'tsu\' ending (archaic)',
            'v2t-s': 'Nidan verb (lower class) with \'tsu\' ending (archaic)',
            'v2w-s': 'Nidan verb (lower class) with \'u\' ending and \'we\' conjugation (archaic)',
            'v2y-k': 'Nidan verb (upper class) with \'yu\' ending (archaic)',
            'v2y-s': 'Nidan verb (lower class) with \'yu\' ending (archaic)',
            'v2z-s': 'Nidan verb (lower class) with \'zu\' ending (archaic)',
            'v4b': 'Yodan verb with \'bu\' ending (archaic)',
            'v4g': 'Yodan verb with \'gu\' ending (archaic)',
            'v4h': 'Yodan verb with \'hu/fu\' ending (archaic)',
            'v4k': 'Yodan verb with \'ku\' ending (archaic)',
            'v4m': 'Yodan verb with \'mu\' ending (archaic)',
            'v4n': 'Yodan verb with \'nu\' ending (archaic)',
            'v4r': 'Yodan verb with \'ru\' ending (archaic)',
            'v4s': 'Yodan verb with \'su\' ending (archaic)',
            'v4t': 'Yodan verb with \'tsu\' ending (archaic)',

            # Usage and style tags
            'arch': 'archaic',
            'dated': 'dated term',
            'obs': 'obsolete term',
            'rare': 'rare term',
            'form': 'formal or literary term',
            'col': 'colloquial',
            'fam': 'familiar language',
            'sl': 'slang',
            'm-sl': 'manga slang',
            'net-sl': 'Internet slang',
            'hon': 'honorific or respectful (sonkeigo) language',
            'hum': 'humble (kenjougo) language',
            'pol': 'polite (teineigo) language',
            'male': 'male term or language',
            'fem': 'female term or language',
            'chn': 'children\'s language',
            'joc': 'jocular, humorous term',
            'vulg': 'vulgar expression or word',
            'derog': 'derogatory',
            'X': 'rude or X-rated term (not displayed in educational software)',
            'sens': 'sensitive',
            'euph': 'euphemistic',

            # Kanji and kana usage
            'sK': 'search-only kanji form',
            'rK': 'rarely used kanji form',
            'iK': 'word containing irregular kanji usage',
            'oK': 'word containing out-dated kanji or kanji usage',
            'sk': 'search-only kana form',
            'rk': 'rarely used kana form',
            'ik': 'word containing irregular kana usage',
            'ok': 'out-dated or obsolete kana usage',
            'uk': 'word usually written using kana alone',
            'io': 'irregular okurigana usage',
            'ateji': 'ateji (phonetic) reading',
            'gikun': 'gikun (meaning as reading) or jukujikun (special kanji reading)',

            # Specialized fields
            'abbr': 'abbreviation',
            'anat': 'anatomy',
            'agric': 'agriculture',
            'archeol': 'archeology',
            'archit': 'architecture',
            'art': 'art, aesthetics',
            'astron': 'astronomy',
            'audvid': 'audiovisual',
            'aviat': 'aviation',
            'baseb': 'baseball',
            'biochem': 'biochemistry',
            'biol': 'biology',
            'bot': 'botany',
            'boxing': 'boxing',
            'Buddh': 'Buddhism',
            'bus': 'business',
            'cards': 'card games',
            'chem': 'chemistry',
            'Christn': 'Christianity',
            'civeng': 'civil engineering',
            'cloth': 'clothing',
            'comp': 'computing',
            'cryst': 'crystallography',
            'dent': 'dentistry',
            'ecol': 'ecology',
            'econ': 'economics',
            'elec': 'electricity, elec. eng.',
            'electr': 'electronics',
            'embryo': 'embryology',
            'engr': 'engineering',
            'ent': 'entomology',
            'figskt': 'figure skating',
            'film': 'film',
            'finc': 'finance',
            'fish': 'fishing',
            'food': 'food, cooking',
            'gardn': 'gardening, horticulture',
            'genet': 'genetics',
            'geogr': 'geography',
            'geol': 'geology',
            'geom': 'geometry',
            'go': 'go (game)',
            'golf': 'golf',
            'gramm': 'grammar',
            'hanaf': 'hanafuda',
            'hist': 'historical term',
            'horse': 'horse racing',
            'internet': 'Internet',
            'kabuki': 'kabuki',
            'law': 'law',
            'ling': 'linguistics',
            'logic': 'logic',
            'MA': 'martial arts',
            'mahj': 'mahjong',
            'manga': 'manga',
            'math': 'mathematics',
            'mech': 'mechanical engineering',
            'med': 'medicine',
            'met': 'meteorology',
            'mil': 'military',
            'min': 'mineralogy',
            'mining': 'mining',
            'motor': 'motorsport',
            'music': 'music',
            'myth': 'mythology',
            'jpmyth': 'Japanese mythology',
            'grmyth': 'Greek mythology',
            'rommyth': 'Roman mythology',
            'chmyth': 'Chinese mythology',
            'noh': 'noh',
            'ornith': 'ornithology',
            'paleo': 'paleontology',
            'pathol': 'pathology',
            'pharm': 'pharmacology',
            'phil': 'philosophy',
            'photo': 'photography',
            'physics': 'physics',
            'physiol': 'physiology',
            'poet': 'poetical term',
            'politics': 'politics',
            'print': 'printing',
            'prowres': 'professional wrestling',
            'psych': 'psychology',
            'psyanal': 'psychoanalysis',
            'psy': 'psychiatry',
            'rail': 'railway',
            'relig': 'religion',
            'shogi': 'shogi',
            'Shinto': 'Shinto',
            'ski': 'skiing',
            'sports': 'sports',
            'stat': 'statistics',
            'stockm': 'stock market',
            'sumo': 'sumo',
            'surg': 'surgery',
            'telec': 'telecommunications',
            'tv': 'television',
            'vet': 'veterinary terms',
            'vidg': 'video games',
            'zool': 'zoology',

            # Mimetics and expressions
            'on-mim': 'onomatopoeic or mimetic word',
            'yoji': 'yojijukugo',
            'proverb': 'proverb',
            'quote': 'quotation',

            # Proper noun types
            'person': 'full name of a particular person',
            'given': 'given name or forename, gender not specified',
            'surname': 'family or surname',
            'place': 'place name',
            'station': 'railway station',
            'company': 'company name',
            'organization': 'organization name',
            'product': 'product name',
            'work': 'work of art, literature, music, etc. name',
            'char': 'character',
            'creat': 'creature',
            'dei': 'deity',
            'ev': 'event',
            'fict': 'fiction',
            'group': 'group',
            'leg': 'legend',
            'obj': 'object',
            'oth': 'other',
            'serv': 'service',
            'ship': 'ship name',
            'unc': 'unclassified',
            'unclass': 'unclassified name',
            'doc': 'document',
            'tradem': 'trademark',

            # Dialectal
            'hob': 'Hokkaido-ben',
            'thb': 'Touhoku-ben',
            'ktb': 'Kantou-ben',
            'kyb': 'Kyoto-ben',
            'ksb': 'Kansai-ben',
            'osb': 'Osaka-ben',
            'tsb': 'Tosa-ben',
            'tsug': 'Tsugaru-ben',
            'kyu': 'Kyuushuu-ben',
            'rkb': 'Ryuukyuu-ben',
            'nab': 'Nagano-ben',
            'bra': 'Brazilian',
            'masc': 'male name',
        }

        for tag, description in tag_definitions.items():
            cursor.execute("""
                INSERT OR REPLACE INTO tag_definitions (tag, description)
                VALUES (?, ?)
            """, (tag, description))

        conn.commit()
        print(f"‚úÖ Added {len(tag_definitions)} tag definitions")

    def populate_pitch_accents(self, conn: sqlite3.Connection) -> None:
        """Populate pitch accent table from accents.json"""
        cursor = conn.cursor()
        
        accents_file = os.path.join(os.path.dirname(__file__), 'app', 'src', 'main', 'assets', 'accents.json')
        
        if not os.path.exists(accents_file):
            print("‚ö†Ô∏è  accents.json not found, skipping pitch accent data")
            return
            
        print("üéµ Populating pitch accent data...")
        
        try:
            with open(accents_file, 'r', encoding='utf-8') as f:
                accent_data = json.load(f)
            
            if 'accents' not in accent_data:
                print("‚ùå Invalid accent data format")
                return
                
            count = 0
            skipped = 0
            
            for kanji_form, readings_dict in accent_data['accents'].items():
                for reading, accent_numbers in readings_dict.items():
                    try:
                        # Create accent pattern string (e.g., "1,2" for multiple patterns)
                        accent_pattern = ','.join(map(str, accent_numbers))
                        
                        cursor.execute("""
                            INSERT OR REPLACE INTO pitch_accents 
                            (kanji_form, reading, accent_pattern)
                            VALUES (?, ?, ?)
                        """, (kanji_form, reading, accent_pattern))
                        
                        count += 1
                        
                        if count % 10000 == 0:
                            print(f"   üìä Processed {count:,} accent entries...")
                            
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Skipping accent entry {kanji_form}:{reading} - {e}")
                        skipped += 1
                        continue
            
            conn.commit()
            print(f"‚úÖ Added {count:,} pitch accent entries")
            if skipped > 0:
                print(f"   ‚ö†Ô∏è  Skipped {skipped} invalid entries")
                
        except Exception as e:
            print(f"‚ùå Error loading pitch accent data: {e}")
    
    def populate_word_variants(self, conn: sqlite3.Connection) -> None:
        """Populate word variants table from jmdict.json"""
        cursor = conn.cursor()
        
        jmdict_file = os.path.join(os.path.dirname(__file__), 'app', 'src', 'main', 'assets', 'jmdict.json')
        
        if not os.path.exists(jmdict_file):
            print("‚ö†Ô∏è  jmdict.json not found, skipping word variants")
            return
        
        print("üîó Populating word variants...")
        
        try:
            with open(jmdict_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            count = 0
            bidirectional_count = 0
            skipped = 0
            
            for entry in data['words']:
                # Only process entries with multiple kanji forms (variants)
                if 'kanji' in entry and len(entry['kanji']) > 1:
                    jmdict_id = entry.get('id', 'unknown')
                    
                    # Get the first reading and meaning for the variants
                    reading = entry['kana'][0]['text'] if 'kana' in entry and entry['kana'] else ''
                    
                    # Get first meaning
                    meaning = ''
                    if 'sense' in entry and entry['sense']:
                        for sense in entry['sense']:
                            if 'gloss' in sense:
                                for gloss in sense['gloss']:
                                    if 'text' in gloss:
                                        meaning = gloss['text']
                                        break
                                if meaning:
                                    break
                    
                    # Extract all kanji forms
                    kanji_forms = [k['text'] for k in entry['kanji']]
                    
                    # Create bidirectional variant relationships
                    # Each kanji form is linked to all other forms as variants
                    for i, primary_kanji in enumerate(kanji_forms):
                        for j, variant_kanji in enumerate(kanji_forms):
                            if i != j:  # Don't link a word to itself
                                try:
                                    cursor.execute("""
                                        INSERT OR IGNORE INTO word_variants 
                                        (jmdict_id, primary_kanji, variant_kanji, reading, meaning)
                                        VALUES (?, ?, ?, ?, ?)
                                    """, (jmdict_id, primary_kanji, variant_kanji, reading, meaning))
                                    
                                    bidirectional_count += 1
                                    
                                except Exception as e:
                                    # Skip duplicates or errors
                                    skipped += 1
                                    continue
                    
                    count += 1
                    
                    if count % 1000 == 0:
                        print(f"   üìä Processed {count:,} variant groups ({bidirectional_count:,} relationships)...")
            
            conn.commit()
            print(f"‚úÖ Added {count:,} variant groups with {bidirectional_count:,} bidirectional relationships")
            if skipped > 0:
                print(f"   ‚ö†Ô∏è  Skipped {skipped} duplicate entries")
            
        except Exception as e:
            print(f"‚ùå Error loading word variants: {e}")

    def generate_ngrams(self, text: str) -> list[str]:
        """Generate 2-, 3-, and 4-character n-grams from text for substring search"""
        if not text or len(text) < 2:
            return []
        
        ngrams = []
        for n in [2, 3, 4]:  # Generate 2-gram, 3-gram, 4-gram
            for i in range(len(text) - n + 1):
                ngram = text[i:i+n]
                ngrams.append(ngram)
        return ngrams

    def normalize_romaji(self, text: str) -> str:
        """
        Optional: Normalize Romaji characters like '≈ç' (U+014D) to simpler ASCII 'o'.
        Add this function to fields that might contain such characters if desired.
        """
        if not text:
            return ""
        # Example: Replace macrons with their base ASCII equivalents
        text = text.replace('ƒÅ', 'a').replace('ƒ´', 'i').replace('≈´', 'u').replace('ƒì', 'e').replace('≈ç', 'o')
        text = text.replace('ƒÄ', 'A').replace('ƒ™', 'I').replace('≈™', 'U').replace('ƒí', 'E').replace('≈å', 'O')
        # Add any other specific character replacements here if needed (e.g., apostrophes if they are mis-parsed)
        # text = text.replace("'", "") # Example: To remove straight apostrophes if undesired
        # text = text.replace("'", "") # Example: To remove curly apostrophes if undesired
        return text

    def tokenize_japanese(self, text: str) -> str:
        """
        Tokenize Japanese text for FTS5 search (removes spacing after tokenization)

        This function:
        1. Uses MeCab to properly tokenize compound words (e.g., "„Åø„Çã„Åπ„Åç" -> "„Åø„Çã" + "„Åπ„Åç")
        2. Removes spaces between tokens so FTS5 can find substring matches
        """
        if not text:
            return ""

        if self.mecab_tokenizer:
            try:
                node = self.mecab_tokenizer.parseToNode(text)
                tokens = []

                while node:
                    if node.surface:
                        tokens.append(node.surface)
                    node = node.next

                # IMPORTANT: Remove spaces after tokenization
                # Example: "„Åø„Çã„Åπ„Åç" -> ["„Åø„Çã", "„Åπ„Åç"] -> "„Åø„Çã„Åπ„Åç" (no spaces)
                return "".join(tokens)
            except Exception as e:
                print(f"‚ö†Ô∏è  MeCab tokenization failed for '{text}': {e}. Falling back to simple text.")

        # Fallback: basic character-level tokenization (no spaces needed as it's not tokenized)
        return text

    def calculate_frequency(self, word: str, is_common: bool, kana_reading: str = None, parts_of_speech: list = None) -> int:
        """
        Calculate frequency score for word.
        Assigns 0 frequency to known "pure proper noun" types to avoid false high rankings.
        """
        if parts_of_speech:
            # These are common JMnedict tags that indicate a proper name, not a dictionary word.
            pure_proper_noun_name_types = {
                'fem', 'masc', 'given', 'surname', 'person', 'char', 'place', 'station',
                'company', 'organization', 'group', 'product', 'serv', 'work', 'ev', 'obj',
                'creat', 'dei', 'myth', 'leg', 'ship', 'doc', 'relig', 'fict', 'oth', 'unclass'
            }

            # Check if any of the entry's POS/name tags match our pure proper noun types.
            if any(tag in pure_proper_noun_name_types for tag in parts_of_speech):
                return 0 # Assign 0 frequency to avoid contaminating general word frequencies.

        # Use exact matches from loaded frequency data.
        if word in self.frequency_data:
            return self.frequency_data[word]

        # No exact match found - return 0.
        return 0

    def load_jmdict_data(self, file_path: str) -> List[Dict]:
        """Load JMdict data from JSON file"""
        print(f"üìñ Loading JMdict data from {file_path}...")

        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Handle new JMdict format with metadata and words array
                if isinstance(data, dict) and 'words' in data:
                    words = data['words']
                    print(f"‚úÖ Loaded {len(words)} entries from JMdict file (new format)")
                    return words
                elif isinstance(data, list):
                    # Old format - direct array of entries
                    print(f"‚úÖ Loaded {len(data)} entries from JMdict file (old format)")
                    return data
                else:
                    print(f"‚ùå Unexpected JMdict format")
                    return []
            except Exception as e:
                print(f"‚ùå Failed to load JMdict data from {file_path}: {e}")
                return []
        else:
            print(f"‚ùå JMdict file not found at {file_path}")
            print("Please ensure you have jmdict.json file in the assets directory")
            return []

    def load_tags_data(self, tags_file_path: str) -> Dict[str, List[str]]:
        """Load tags data from tags.json file (JMdict POS tags)"""
        print(f"üè∑Ô∏è  Loading tags data from {tags_file_path}...")

        if not os.path.exists(tags_file_path):
            print(f"‚ö†Ô∏è  No JMdict tags file found at {tags_file_path}")
            return {}

        try:
            with open(tags_file_path, 'r', encoding='utf-8') as f:
                tag_data = json.load(f)

            # Process the tags data (JMdict structure)
            all_tags = {}
            for word, tag_info in tag_data.items():
                if isinstance(tag_info, dict) and 'senses' in tag_info:
                    pos_tags = []
                    for sense in tag_info['senses']:
                        if 'pos' in sense:
                            pos_tags.extend(sense['pos'])

                    if pos_tags:
                        all_tags[word] = pos_tags

            print(f"‚úÖ Loaded tags for {len(all_tags)} words from JMdict tags file")
            return all_tags
        except Exception as e:
            print(f"‚ùå Failed to load tags file: {e}")
            return {}

    def populate_dictionary_entries(self, conn: sqlite3.Connection, entries_data: List[Dict], tags_data: Dict[str, List[str]]) -> None:
        """Populate the main dictionary_entries table and word_tags table"""
        cursor = conn.cursor()

        print("üìù Populating dictionary entries...")
        entries_added = 0
        tags_added = 0
        error_count = 0  # Track SQL errors
        max_errors = 100  # Maximum allowed errors before stopping


        batch_size = 1000  # Smaller batch size to prevent corruption
        
        for i, entry in enumerate(entries_data):
            # Debug: Track „Åø„Çã entries specifically
            kana_forms = [k['text'] for k in entry.get('kana', [])]
            if '„Åø„Çã' in kana_forms:
                kanji_forms = [k['text'] for k in entry.get('kanji', [])]
                print(f"   üîç Processing „Åø„Çã entry #{i}: kanji={kanji_forms}, kana={kana_forms}")
            
            # Use smaller batches to prevent database corruption
            if i > 0 and i % batch_size == 0:
                try:
                    conn.commit()
                    # Perform integrity check periodically
                    if i % 10000 == 0:
                        print(f"   Progress: {i}/{len(entries_data)} entries...")
                        check_cursor = conn.execute("PRAGMA integrity_check")
                        result = check_cursor.fetchone()[0]
                        if result != "ok":
                            print(f"   ‚ö†Ô∏è  Database integrity issue detected: {result}")
                            raise sqlite3.DatabaseError("Database integrity check failed")
                except sqlite3.DatabaseError as e:
                    print(f"   ‚ùå Database error during commit: {e}")
                    if "database disk image is malformed" in str(e):
                        print("   üö® CRITICAL: Database corruption detected. Stopping build.")
                        raise
                    # Try to recover by rolling back
                    try:
                        conn.rollback()
                        print("   üîÑ Rolled back transaction, continuing...")
                    except:
                        raise

            # Handle new format with 'sense' array (JMdict)
            meanings = []
            parts_of_speech_list = []
            
            if 'sense' in entry:
                # JMdict format
                for sense in entry.get('sense', []):
                    # Extract meanings from gloss array
                    for gloss in sense.get('gloss', []):
                        if isinstance(gloss, dict):
                            # Standard JMdict format with lang field, or our custom format without lang
                            if gloss.get('lang') == 'eng' or not gloss.get('lang'):
                                text = gloss.get('text', '')
                                if text:
                                    meanings.append(text)
                    
                    # Extract parts of speech from each sense
                    # Support both 'partOfSpeech' (standard JMdict) and 'pos' (our custom entries)
                    pos_tags = sense.get('partOfSpeech', []) or sense.get('pos', [])
                    parts_of_speech_list.extend(pos_tags)
            elif 'translation' in entry:
                # JMnedict format
                for trans in entry.get('translation', []):
                    # Extract meanings from translation array
                    for t in trans.get('translation', []):
                        if isinstance(t, dict) and t.get('lang') == 'eng':
                            text = t.get('text', '')
                            if text:
                                meanings.append(text)
                    
                    # Extract name types as parts of speech
                    name_types = trans.get('type', [])
                    parts_of_speech_list.extend(name_types)
                    
                    # Debug logging for JMnedict tag extraction
                    if is_jmnedict_source:
                        kanji_forms_debug = [k['text'] for k in entry.get('kanji', [])]
                        if '‰∏ä' in kanji_forms_debug:
                            print(f"   üîç JMnedict tag extraction for ‰∏ä: name_types={name_types}")
            else:
                # Old format fallback
                meanings = entry.get('meanings', [])
            
            if not meanings:
                continue

            meanings_json = json.dumps(meanings)

            kanji_forms = [k['text'] for k in entry.get('kanji', [])]
            kana_forms = [k['text'] for k in entry.get('kana', [])]

            # Collect additional POS tags from tags file (if not already populated from sense)
            if not parts_of_speech_list:
                for word in kanji_forms + kana_forms:
                    if word in tags_data:
                        parts_of_speech_list.extend(tags_data[word])
            
            # Remove duplicates, preserve order
            parts_of_speech_list = list(dict.fromkeys(parts_of_speech_list))
            parts_of_speech_json = json.dumps(parts_of_speech_list)

            # is_common will be calculated per form now (not at entry level)

            # Build forms with their metadata
            forms_to_insert = []
            kanji_entries = entry.get('kanji', [])
            kana_entries = entry.get('kana', [])
            
            if kanji_forms:
                for kanji_text in kanji_forms:
                    # Find the kanji entry object for this text
                    kanji_entry = next((k for k in kanji_entries if k['text'] == kanji_text), {})
                    kanji_common = kanji_entry.get('common', False)
                    kanji_tags = kanji_entry.get('tags', [])  # Extract kanji-level tags (rK, iK, etc.)
                    
                    for kana_text in kana_forms:
                        # Find the kana entry object for this text
                        kana_entry = next((k for k in kana_entries if k['text'] == kana_text), {})
                        kana_common = kana_entry.get('common', False)
                        kana_tags = kana_entry.get('tags', [])  # Extract kana-level tags (rk, ik, etc.)
                        
                        # Form is common if both kanji and kana are common
                        is_common = kanji_common and kana_common
                        
                        # Combine kanji and kana tags for this form
                        form_tags = kanji_tags + kana_tags
                        
                        forms_to_insert.append((kanji_text, kana_text, is_common, form_tags))
            elif kana_forms:
                for kana_text in kana_forms:
                    # Find the kana entry object for this text
                    kana_entry = next((k for k in kana_entries if k['text'] == kana_text), {})
                    kana_common = kana_entry.get('common', False)
                    kana_tags = kana_entry.get('tags', [])  # Extract kana-level tags
                    
                    forms_to_insert.append((None, kana_text, kana_common, kana_tags))

            for kanji_form, kana_form, is_common, form_tags in forms_to_insert:
                # OPTIONAL: Apply Romaji normalization here if desired
                # normalized_kana_form = self.normalize_romaji(kana_form)
                # normalized_kanji_form = self.normalize_romaji(kanji_form) if kanji_form else None

                tokenized_kanji = self.tokenize_japanese(kanji_form) if kanji_form else None
                tokenized_reading = self.tokenize_japanese(kana_form)

                frequency = self.calculate_frequency(kanji_form if kanji_form else kana_form, is_common, kana_form, parts_of_speech_list)

                try:
                    # Both JMdict and JMNEdict entries should be inserted - they serve different purposes
                    # JMdict = dictionary words, JMNEdict = proper nouns/names
                    # The UNIQUE constraint is on (kanji, reading) but we want both types when they exist
                    
                    # Debug logging for „Åø„Çã entries
                    if kana_form == '„Åø„Çã':
                        print(f"   üîç DEBUG: Inserting „Åø„Çã entry:")
                        print(f"      kanji_form: {repr(kanji_form)}")
                        print(f"      kana_form: {repr(kana_form)}")
                        print(f"      frequency: {frequency}")
                        print(f"      is_common: {is_common}")
                        print(f"      parts_of_speech: {parts_of_speech_list}")
                    
                    cursor.execute("""
                        INSERT OR IGNORE INTO dictionary_entries
                        (kanji, reading, meanings, parts_of_speech, is_common,
                         frequency, tokenized_kanji, tokenized_reading)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (kanji_form, kana_form, meanings_json, parts_of_speech_json,
                          1 if is_common else 0, frequency, tokenized_kanji, tokenized_reading))

                    # Get the entry ID - check if entry was actually inserted
                    if cursor.rowcount > 0:
                        # Entry was inserted successfully
                        entry_id = cursor.lastrowid
                    else:
                        # INSERT was ignored (duplicate), fetch the existing entry's ID
                        cursor.execute("""
                            SELECT id FROM dictionary_entries 
                            WHERE reading = ? AND (kanji IS ? OR (kanji IS NULL AND ? IS NULL))
                        """, (kana_form, kanji_form, kanji_form))
                        result = cursor.fetchone()
                        entry_id = result[0] if result else None
                    
                    # Debug logging for „Åø„Çã entries - check if insertion was successful
                    if kana_form == '„Åø„Çã':
                        if entry_id:
                            print(f"   ‚úÖ DEBUG: „Åø„Çã entry inserted successfully with ID: {entry_id}")
                        else:
                            print(f"   ‚ùå DEBUG: „Åø„Çã entry insertion was ignored (likely duplicate)")
                            # Check what existing entry conflicts
                            cursor.execute("""
                                SELECT id, kanji, reading, frequency, is_common 
                                FROM dictionary_entries 
                                WHERE reading = ? AND (kanji IS ? OR (kanji IS NULL AND ? IS NULL))
                            """, (kana_form, kanji_form, kanji_form))
                            existing = cursor.fetchone()
                            if existing:
                                print(f"      Existing entry: ID={existing[0]}, kanji={repr(existing[1])}, freq={existing[3]}, is_common={existing[4]}")
                            else:
                                print(f"      No conflicting entry found - insertion failed for other reason")


                    if entry_id:
                        # Insert parts of speech tags
                        for tag in parts_of_speech_list:
                            try:
                                cursor.execute("""
                                    INSERT OR REPLACE INTO word_tags (entry_id, tag)
                                    VALUES (?, ?)
                                """, (entry_id, tag))
                                tags_added += 1
                                
                            except sqlite3.Error as e:
                                pass
                        
                        # Insert kanji/kana form-specific tags (rK, iK, rk, ik, etc.)
                        for tag in form_tags:
                            try:
                                cursor.execute("""
                                    INSERT OR REPLACE INTO word_tags (entry_id, tag)
                                    VALUES (?, ?)
                                """, (entry_id, tag))
                                tags_added += 1
                                
                                # Debug logging for „ÇÇ„Å¶„ÅÇ„Åù„Å∂ entries
                                if kana_form == '„ÇÇ„Å¶„ÅÇ„Åù„Å∂':
                                    print(f"         ‚úÖ Inserted form tag: {tag} for {kanji_form}/{kana_form}")
                            except sqlite3.Error as e:
                                if kana_form == '„ÇÇ„Å¶„ÅÇ„Åù„Å∂':
                                    print(f"         ‚ùå Failed to insert form tag: {tag} for {kanji_form}/{kana_form}, error: {e}")
                                pass
                    else:
                        pass
                    entries_added += 1
                except sqlite3.Error as e:
                    error_count += 1
                    print(f"   ‚ö†Ô∏è  SQL Error inserting {kanji_form}/{kana_form}: {e}")
                    if "database disk image is malformed" in str(e):
                        print(f"   üö® CRITICAL: Database corruption detected after {i} entries")
                        raise
                    if error_count > max_errors:
                        print(f"   üö® Too many errors ({error_count}), stopping build")
                        raise RuntimeError(f"Too many SQL errors: {error_count}")
                    continue

        # Final commit
        try:
            conn.commit()
            print(f"‚úÖ Added {entries_added} dictionary entries from this source")
            print(f"‚úÖ Added {tags_added} word tags from this source")
            if error_count > 0:
                print(f"   ‚ö†Ô∏è  Encountered {error_count} errors during insertion")
        except sqlite3.DatabaseError as e:
            print(f"   ‚ùå Final commit failed: {e}")
            raise

    def populate_fts_tables(self, conn: sqlite3.Connection) -> None:
        """Populate FTS5 virtual tables"""
        cursor = conn.cursor()

        print("üîç Populating FTS5 tables...")

        # Populate entries_fts5 table
        # This will automatically pull the new columns (is_common, frequency, is_jmnedict_entry)
        # because they are specified in the CREATE VIRTUAL TABLE DDL.
        # FTS5 'rebuild' command ensures all data from the content table is re-indexed.
        cursor.execute("""
            INSERT INTO entries_fts5(entries_fts5) VALUES('rebuild')
        """)

        # Populate english_fts table (no changes needed here as it doesn't use the new columns)
        cursor.execute("""
            INSERT INTO english_fts (entry_id, meanings, parts_of_speech)
            SELECT id, meanings, parts_of_speech FROM dictionary_entries
        """)

        # Populate japanese_substring_fts table with individual n-gram tokens
        print("   üìù Generating n-grams for substring search...")
        cursor.execute("SELECT id, reading, kanji FROM dictionary_entries WHERE reading IS NOT NULL OR kanji IS NOT NULL")
        entries = cursor.fetchall()
        
        ngram_count = 0
        total_ngrams = 0
        
        for entry_id, reading, kanji in entries:
            entry_ngrams = set()  # Use set to avoid duplicate n-grams for same entry
            
            # Generate n-grams for reading
            if reading:
                reading_ngrams = self.generate_ngrams(reading)
                entry_ngrams.update(reading_ngrams)
            
            # Generate n-grams for kanji
            if kanji:
                kanji_ngrams = self.generate_ngrams(kanji)
                entry_ngrams.update(kanji_ngrams)
            
            # Insert each n-gram as a separate row
            for ngram in entry_ngrams:
                cursor.execute("""
                    INSERT INTO japanese_substring_fts (entry_id, ngram)
                    VALUES (?, ?)
                """, (entry_id, ngram))
                total_ngrams += 1
            
            ngram_count += 1
            if ngram_count % 5000 == 0:
                print(f"      Generated n-grams for {ngram_count:,} entries ({total_ngrams:,} total n-grams)...")
        
        print(f"   ‚úÖ Generated n-grams for {ngram_count:,} entries ({total_ngrams:,} total n-grams)")

        conn.commit()
        print("‚úÖ FTS5 tables populated (including japanese substring search)")

    def create_triggers(self, conn: sqlite3.Connection) -> None:
        """Create triggers to keep FTS5 tables in sync"""
        cursor = conn.cursor()

        print("üîß Creating triggers...")

        # Trigger for entries_fts5 (AFTER INSERT)
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS entries_fts5_ai AFTER INSERT ON dictionary_entries
            BEGIN
                INSERT INTO entries_fts5(rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency)
                VALUES (new.id, new.kanji, new.reading, new.meanings, new.tokenized_kanji,
                        new.tokenized_reading, new.parts_of_speech,
                        new.is_common, new.frequency);
            END
        """)

        # Trigger for entries_fts5 (AFTER DELETE)
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS entries_fts5_ad AFTER DELETE ON dictionary_entries
            BEGIN
                INSERT INTO entries_fts5(entries_fts5, rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency)
                VALUES ('delete', old.id, old.kanji, old.reading, old.meanings, old.tokenized_kanji,
                        old.tokenized_reading, old.parts_of_speech,
                        old.is_common, old.frequency);
            END
        """)

        # Trigger for entries_fts5 (AFTER UPDATE)
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS entries_fts5_au AFTER UPDATE ON dictionary_entries
            BEGIN
                INSERT INTO entries_fts5(entries_fts5, rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency)
                VALUES ('delete', old.id, old.kanji, old.reading, old.meanings, old.tokenized_kanji,
                        old.tokenized_reading, old.parts_of_speech,
                        old.is_common, old.frequency);
                INSERT INTO entries_fts5(rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency)
                VALUES (new.id, new.kanji, new.reading, new.meanings, new.tokenized_kanji,
                        new.tokenized_reading, new.parts_of_speech,
                        new.is_common, new.frequency);
            END
        """)

        # english_fts triggers remain the same as they don't use these new columns
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS english_fts_ai AFTER INSERT ON dictionary_entries
            BEGIN
                INSERT INTO english_fts(entry_id, meanings, parts_of_speech)
                VALUES (new.id, new.meanings, new.parts_of_speech);
            END
        """)

        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS english_fts_ad AFTER DELETE ON dictionary_entries
            BEGIN
                DELETE FROM english_fts WHERE entry_id = old.id;
            END
        """)

        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS english_fts_au AFTER UPDATE ON dictionary_entries
            BEGIN
                UPDATE english_fts SET meanings = new.meanings, parts_of_speech = new.parts_of_speech
                WHERE entry_id = old.id;
            END
        """)

        print("‚úÖ Triggers created")

    def verify_database(self, conn: sqlite3.Connection) -> bool:
        """Verify the database was created correctly"""
        cursor = conn.cursor()

        print("üîç Verifying database...")

        # Check table counts
        cursor.execute("SELECT COUNT(*) FROM dictionary_entries")
        entry_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM entries_fts5")
        fts5_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM english_fts")
        english_fts_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM tag_definitions")
        tag_defs_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM word_tags")
        word_tags_count = cursor.fetchone()[0]

        # Check kanji tables too
        cursor.execute("SELECT COUNT(*) FROM kanji_entries")
        kanji_count = cursor.fetchone()[0]

        print(f"   Dictionary entries: {entry_count}")
        print(f"   FTS5 entries: {fts5_count}")
        print(f"   English FTS entries: {english_fts_count}")
        print(f"   Tag definitions: {tag_defs_count}")
        print(f"   Word tags: {word_tags_count}")
        print(f"   Kanji entries: {kanji_count}")

        # Test search functionality
        cursor.execute("""
            SELECT COUNT(*) FROM dictionary_entries
            WHERE id IN (
                SELECT rowid FROM entries_fts5
                WHERE reading MATCH ? OR tokenized_reading MATCH ?
            )
        """, ('„Åø„Çã', '„Åø„Çã'))

        search_results = cursor.fetchone()[0]
        print(f"   Search test („Åø„Çã): {search_results} results")

        # Test tag functionality
        cursor.execute("""
            SELECT COUNT(*) FROM word_tags wt
            JOIN tag_definitions td ON wt.tag = td.tag
            WHERE wt.tag = 'v1'
        """)
        tag_test = cursor.fetchone()[0]
        print(f"   Tag test (v1 verbs): {tag_test} results")

        # Test kanji lookup
        cursor.execute("""
            SELECT COUNT(*) FROM kanji_entries
            WHERE kanji = 'Ê∞¥'
        """)
        kanji_test = cursor.fetchone()[0]
        print(f"   Kanji test ('Ê∞¥'): {kanji_test} results")

        # Check if we have reasonable data (FIXED: tag_defs_count >= 50)
        if (entry_count > 100000 and fts5_count == entry_count and
            search_results > 0 and tag_defs_count >= 50 and word_tags_count > 100000):
            print("‚úÖ Database verification passed")
            return True
        else:
            print("‚ùå Database verification failed")
            return False

    def create_kanjidic_schema(self, conn: sqlite3.Connection) -> None:
        """Create schema for KanjiDic database"""
        cursor = conn.cursor()

        print("üìã Creating KanjiDic database schema...")

        # Main kanji entries table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS kanji_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                kanji TEXT NOT NULL UNIQUE,
                jlpt_level INTEGER,
                grade INTEGER,
                stroke_count INTEGER,
                frequency INTEGER,
                meanings TEXT,
                kun_readings TEXT,
                on_readings TEXT,
                nanori_readings TEXT,
                radical_names TEXT,
                heisig_number INTEGER,
                heisig_keyword TEXT,
                components TEXT,
                radical TEXT,
                radical_number INTEGER
            )
        """)

        # Create indexes
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_kanji ON kanji_entries(kanji)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_jlpt ON kanji_entries(jlpt_level)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_grade ON kanji_entries(grade)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_frequency ON kanji_entries(frequency)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_stroke_count ON kanji_entries(stroke_count)")

        # Kanji radical mapping tables (from kradfile.json)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS kanji_radical_mapping (
                kanji TEXT PRIMARY KEY,
                components TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS radical_kanji_mapping (
                radical TEXT PRIMARY KEY,
                stroke_count INTEGER,
                kanji_list TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS radical_decomposition_mapping (
                radical TEXT PRIMARY KEY,
                components TEXT NOT NULL,
                component_count INTEGER NOT NULL
            )
        """)

        # Create indexes for radical tables
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_radical_stroke_count ON radical_kanji_mapping(stroke_count)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_decomposition_component_count ON radical_decomposition_mapping(component_count)")

        # No FTS5 tables needed for kanji - direct lookup is sufficient

        print("‚úÖ KanjiDic database schema created")

    def load_kradfile_data(self, file_path: str) -> Optional[Dict]:
        """Load kradfile data from JSON file"""
        print(f"üìñ Loading kradfile data from {file_path}...")
        
        if not os.path.exists(file_path):
            print(f"‚ö†Ô∏è  kradfile not found: {file_path}")
            return None
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            kanji_data = data.get('kanji', {})
            print(f"‚úÖ Loaded {len(kanji_data)} kanji entries from kradfile")
            return kanji_data
            
        except Exception as e:
            print(f"üö® Error loading kradfile data: {e}")
            return None

    def load_radkfile_data(self, file_path: str) -> Optional[Dict]:
        """Load radkfile data from JSON file"""
        print(f"üìñ Loading radkfile data from {file_path}...")
        
        if not os.path.exists(file_path):
            print(f"‚ö†Ô∏è  radkfile not found: {file_path}")
            return None
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract the radicals data
            radicals_data = data.get('radicals', {})
            print(f"‚úÖ Loaded {len(radicals_data)} radical entries from radkfile")
            return radicals_data
            
        except Exception as e:
            print(f"üö® Error loading radkfile data: {e}")
            return None

    def populate_kanji_radical_mapping(self, conn: sqlite3.Connection, kradfile_data: Dict) -> None:
        """Populate kanji_radical_mapping table from kradfile data"""
        cursor = conn.cursor()
        
        print("üîß Populating kanji radical mapping...")
        
        for kanji, components in kradfile_data.items():
            # Convert list of components to comma-separated string
            components_str = ", ".join(components) if isinstance(components, list) else str(components)
            
            cursor.execute("""
                INSERT OR REPLACE INTO kanji_radical_mapping (kanji, components)
                VALUES (?, ?)
            """, (kanji, components_str))
        
        conn.commit()
        print(f"‚úÖ Added {len(kradfile_data)} kanji component mappings")

    def populate_radical_kanji_mapping_from_kradfile(self, conn: sqlite3.Connection, kradfile_data: Dict, radkfile_data: Dict = None) -> None:
        """Build radical_kanji_mapping table from kradfile data (primary) with radkfile fallback"""
        cursor = conn.cursor()
        
        print("üîß Building radical kanji mapping from kradfile data...")
        
        # Unicode normalization map for radicals (same as preserve_modifications.py)
        unicode_normalization = {
            "ÁÅ¨": "‚∫£",  # Fire radical - normalize to standard form (KEEP - was working)
            "Ëæ∂": "‚ªå",  # Advance radical - kanji form -> radical form (KEEP - was working)
            "\uFA66": "‚ªå",  # Advance radical - compatibility char -> radical form (KEEP - was working)
            "Á§ª": "‚∫≠",  # Spirit radical - kanji form -> radical form (FIXED)
            "ÁΩí": "‚∫≤",  # Net radical - kanji form -> radical form (FIXED)
            "Ê∞µ": "‚∫°",  # Water radical - kanji form -> radical form (FIXED)
            "Áä≠": "‚∫®",  # Dog radical - kanji form -> radical form (FIXED)
            "ÂøÑ": "‚∫ñ",  # Heart radical - kanji form -> radical form (FIXED)
            "Êâå": "‚∫ò",  # Hand radical - kanji form -> radical form (FIXED)
            "Áñí": "‚Ωß",  # Sickness radical - kanji form -> radical form (FIXED)
            "ÂàÇ": "‚∫â",  # Knife radical - kanji form (U+5202) -> radical form (U+2E89) (FIXED)
            "Á¶∏": "‚Ω±",  # Track radical - kanji form -> radical form (FIXED)
            "Ë°§": "‚ªÇ",  # Clothes radical - kanji form -> radical form (FIXED)
        }
        
        # Build radical -> kanji mapping from kradfile
        radical_to_kanji = {}
        
        print("  üìù Inverting kradfile (kanji ‚Üí components) to (radical ‚Üí kanji list)...")
        for kanji, components in kradfile_data.items():
            if isinstance(components, list):
                for radical in components:
                    # Apply Unicode normalization to radicals
                    normalized_radical = unicode_normalization.get(radical, radical)
                    
                    if normalized_radical not in radical_to_kanji:
                        radical_to_kanji[normalized_radical] = []
                    radical_to_kanji[normalized_radical].append(kanji)
                    
                    # Debug spirit radical normalization
                    if radical in ["‚∫≠", "Á§ª"]:
                        print(f"    üîß Normalized '{radical}' ‚Üí '{normalized_radical}' for kanji '{kanji}'")
                    
                    # Debug ‰∫∫ radical specifically
                    if normalized_radical == '‰∫∫' and kanji == 'ÁÅ´':
                        print(f"    üéØ Found ÁÅ´ with ‰∫∫ component in kradfile!")
        
        print(f"  üìä Built mapping for {len(radical_to_kanji)} unique radicals from kradfile")
        
        # Check ‰∫∫ radical from kradfile inversion
        if '‰∫∫' in radical_to_kanji:
            person_kanji_from_krad = radical_to_kanji['‰∫∫']
            has_fire = 'ÁÅ´' in person_kanji_from_krad
            print(f"  üéØ ‰∫∫ radical from kradfile inversion: {len(person_kanji_from_krad)} kanji, contains ÁÅ´: {has_fire}")
        else:
            print("  ‚ùå ‰∫∫ radical not found in kradfile inversion!")
        
        # Get stroke counts from radkfile if available
        radical_stroke_counts = {}
        if radkfile_data:
            print("  üìè Getting stroke counts from radkfile...")
            for radical, info in radkfile_data.items():
                stroke_count = info.get('strokeCount', 0)
                # Apply same normalization to radical keys for stroke count lookup
                normalized_radical = unicode_normalization.get(radical, radical)
                radical_stroke_counts[normalized_radical] = stroke_count
        
        # Insert or update radical mappings
        for radical, kanji_list in radical_to_kanji.items():
            # Get stroke count from radkfile, default to 1 if not found
            stroke_count = radical_stroke_counts.get(radical, 1)
            
            # Convert list of kanji to comma-separated string
            kanji_str = ", ".join(sorted(kanji_list))  # Sort for consistency
            
            cursor.execute("""
                INSERT OR REPLACE INTO radical_kanji_mapping (radical, stroke_count, kanji_list)
                VALUES (?, ?, ?)
            """, (radical, stroke_count, kanji_str))
        
        conn.commit()
        print(f"‚úÖ Built radical mappings for {len(radical_to_kanji)} radicals from kradfile data")
        
        # Add any radicals from radkfile that weren't in kradfile
        if radkfile_data:
            radkfile_only_count = 0
            for radical, info in radkfile_data.items():
                # Apply normalization to check if radical already exists after normalization
                normalized_radical = unicode_normalization.get(radical, radical)
                if normalized_radical not in radical_to_kanji:
                    stroke_count = info.get('strokeCount', 0)
                    kanji_list = info.get('kanji', [])
                    kanji_str = ", ".join(kanji_list) if isinstance(kanji_list, list) else str(kanji_list)
                    
                    cursor.execute("""
                        INSERT OR REPLACE INTO radical_kanji_mapping (radical, stroke_count, kanji_list)
                        VALUES (?, ?, ?)
                    """, (normalized_radical, stroke_count, kanji_str))
                    radkfile_only_count += 1
            
            if radkfile_only_count > 0:
                conn.commit()
                print(f"‚úÖ Added {radkfile_only_count} additional radicals from radkfile")

    def populate_radical_kanji_mapping(self, conn: sqlite3.Connection, radkfile_data: Dict) -> None:
        """Populate radical_kanji_mapping table from radkfile data (legacy method)"""
        cursor = conn.cursor()
        
        print("üîß Populating radical kanji mapping...")
        
        for radical, info in radkfile_data.items():
            stroke_count = info.get('strokeCount', 0)
            kanji_list = info.get('kanji', [])
            
            # Convert list of kanji to comma-separated string
            kanji_str = ", ".join(kanji_list) if isinstance(kanji_list, list) else str(kanji_list)
            
            cursor.execute("""
                INSERT OR REPLACE INTO radical_kanji_mapping (radical, stroke_count, kanji_list)
                VALUES (?, ?, ?)
            """, (radical, stroke_count, kanji_str))
        
        conn.commit()
        print(f"‚úÖ Added {len(radkfile_data)} radical kanji mappings")

    def parse_makemeahanzi_decomposition(self, decomposition: str) -> List[str]:
        """
        Parse makemeahanzi decomposition field, extracting component radicals
        
        Args:
            decomposition: The decomposition string like "‚øª‰∫ÖÂÖ´" or "‚ø∞Ê∞µÈùí"
            
        Returns:
            List of component radicals (without IDC symbols)
        """
        if not decomposition:
            return []

        # IDC (Ideographic Description Characters) to ignore
        idc_chars = {
            '‚ø∞', '‚ø±', '‚ø≤', '‚ø≥', '‚ø¥', '‚øµ', '‚ø∂', '‚ø∑', '‚ø∏', '‚øπ', '‚ø∫', '‚øª'
        }

        # Extract all characters except IDC symbols
        components = []
        for char in decomposition:
            if char not in idc_chars and char.strip():
                components.append(char)

        return components

    def load_makemeahanzi_decomposition_data(self, makemeahanzi_path: str = None) -> Dict[str, List[str]]:
        """
        Load makemeahanzi decomposition data from downloaded dictionary.txt
        
        Args:
            makemeahanzi_path: Optional path to makemeahanzi file, if None will try to download
            
        Returns:
            Dictionary mapping radicals to their component radicals
        """
        print("üìñ Loading makemeahanzi decomposition data...")
        
        # If no path provided, try to download makemeahanzi data
        temp_dir = None
        if not makemeahanzi_path:
            try:
                # Try to use the same download logic from preserve_modifications.py
                import tempfile
                import urllib.request
                import zipfile
                
                temp_dir = tempfile.mkdtemp()
                makemeahanzi_zip = os.path.join(temp_dir, "makemeahanzi.zip")
                makemeahanzi_path = os.path.join(temp_dir, "dictionary.txt")
                
                print("‚¨áÔ∏è  Downloading makemeahanzi data...")
                urllib.request.urlretrieve(
                    "https://github.com/skishore/makemeahanzi/archive/refs/heads/master.zip",
                    makemeahanzi_zip
                )
                
                with zipfile.ZipFile(makemeahanzi_zip, 'r') as zip_ref:
                    zip_ref.extract("makemeahanzi-master/dictionary.txt", temp_dir)
                    
                # Move the extracted file to the expected location
                extracted_path = os.path.join(temp_dir, "makemeahanzi-master", "dictionary.txt")
                if os.path.exists(extracted_path):
                    shutil.move(extracted_path, makemeahanzi_path)
                
                print(f"‚úÖ Downloaded makemeahanzi to {makemeahanzi_path}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not download makemeahanzi data: {e}")
                return {}
        
        if not os.path.exists(makemeahanzi_path):
            print(f"‚ö†Ô∏è  Makemeahanzi file not found at {makemeahanzi_path}")
            return {}
        
        radical_decompositions = {}
        
        try:
            with open(makemeahanzi_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        entry = json.loads(line)
                        character = entry.get("character")
                        decomposition = entry.get("decomposition")
                        
                        if character and decomposition:
                            components = self.parse_makemeahanzi_decomposition(decomposition)
                            if components and len(components) > 1:  # Only include composites
                                radical_decompositions[character] = components
                    
                    except json.JSONDecodeError:
                        continue
                        
        except Exception as e:
            print(f"‚ùå Error parsing makemeahanzi data: {e}")
            return {}
        
        print(f"‚úÖ Parsed {len(radical_decompositions)} radical decompositions from makemeahanzi")
        
        # Clean up temporary files if we downloaded them
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
        
        return radical_decompositions

    def populate_radical_decomposition_mapping(self, conn: sqlite3.Connection, decomposition_data: Dict[str, List[str]], existing_radicals: set) -> None:
        """
        Populate radical_decomposition_mapping table with makemeahanzi decomposition data
        
        Args:
            conn: Database connection
            decomposition_data: Dictionary mapping radicals to component lists
            existing_radicals: Set of radicals that exist in the radical system
        """
        cursor = conn.cursor()
        
        print("üîß Populating radical decomposition mapping...")
        
        # Chinese ‚Üí Japanese radical substitutions
        chinese_to_japanese_substitutions = {
            # Fire and line radicals
            'ÁÅ¨': '‚∫£',  # Chinese fire dots ‚Üí Japanese fire radical
            '‰∏®': 'ÔΩú',  # CJK radical stick ‚Üí Japanese fullwidth vertical line
            
            # Common stroke/shape radicals
            '‰∏ø': '„Éé',  # Left-falling stroke ‚Üí katakana no (similar shape/meaning)
            '‰πö': '‰πô',  # Hook stroke ‚Üí second/hook radical
            '‰∫ª': '‰∫∫',  # Person radical variant ‚Üí person radical
            
            # Note: Âªø and ÂçÑ should decompose naturally:
            # Âªø ‚Üí ['Âªæ', '‰∏Ä'] and ÂçÑ ‚Üí ['ÂçÅ', '‰∏®'] ‚Üí ['ÂçÅ', 'ÔΩú']
            # So no direct substitution needed for these
            
            # Add more substitutions as we research them
        }
        
        # Manual corrections for known issues in makemeahanzi data
        manual_corrections = {
            '‰∏∑': ['‰∏∂', '‰∏∂'],  # Two dots - makemeahanzi has corrupted data with 'Ôºü'
            'ËÇâ': ['ÂÜÇ', '‰∫∫', '‰∫∫'],  # Meat - flattened from ÂÜÇ,‰ªå ‚Üí ÂÜÇ,‰∫∫,‰∫∫
            # Add more corrections here if needed
        }
        
        # Apply manual corrections first
        corrected_count = 0
        for radical, components in manual_corrections.items():
            if radical in existing_radicals:
                valid_components = [comp for comp in components if comp in existing_radicals]
                if valid_components and len(valid_components) > 1:
                    components_str = ",".join(valid_components)
                    component_count = len(valid_components)
                    
                    cursor.execute("""
                        INSERT OR REPLACE INTO radical_decomposition_mapping (radical, components, component_count)
                        VALUES (?, ?, ?)
                    """, (radical, components_str, component_count))
                    
                    corrected_count += 1
                    print(f"    üîß Manual correction: {radical} ‚Üí {valid_components}")
        
        def apply_substitutions_and_expansion(components):
            """Apply Chinese‚ÜíJapanese substitutions and recursive expansion"""
            expanded_components = []
            
            for comp in components:
                # Apply Chinese ‚Üí Japanese substitutions first
                if comp in chinese_to_japanese_substitutions:
                    substitute = chinese_to_japanese_substitutions[comp]
                    expanded_components.append(substitute)
                    print(f"      üîÑ Substituted: {comp} ‚Üí {substitute}")
                # Recursive expansion for missing components that have decompositions
                elif comp not in existing_radicals and comp in decomposition_data:
                    # Recursively expand missing component
                    sub_components = decomposition_data[comp]
                    # Apply substitutions recursively (but avoid infinite loops)
                    if comp != '‰ªå':  # Prevent infinite recursion for ice radical
                        recursive_expansion = apply_substitutions_and_expansion(sub_components)
                        expanded_components.extend(recursive_expansion)
                        print(f"      üîÑ Expanded: {comp} ‚Üí {recursive_expansion}")
                    else:
                        # Special case for ice radical: ‰ªå ‚Üí ['‰∫∫', '‰∫∫']
                        expanded_components.extend(['‰∫∫', '‰∫∫'])
                        print(f"      üîÑ Expanded ice: {comp} ‚Üí ['‰∫∫', '‰∫∫']")
                else:
                    # Keep component as-is
                    expanded_components.append(comp)
            
            return expanded_components
        
        valid_decompositions = 0
        substitution_count = 0
        for radical, components in decomposition_data.items():
            # Skip if we already applied a manual correction
            if radical in manual_corrections:
                continue
                
            # Only include decompositions where the radical exists in our radical system
            if radical in existing_radicals:
                # Apply substitutions and recursive expansion
                expanded_components = apply_substitutions_and_expansion(components)
                
                # Filter to only include components that exist as radicals after expansion
                valid_components = [comp for comp in expanded_components if comp in existing_radicals]
                
                # Allow single components for important hierarchical radicals, otherwise require 2+
                important_single_decompositions = {'ÂÜÇ', '‚∫£'}  # Box and fire radical create useful hierarchical paths
                min_components = 1 if radical in important_single_decompositions else 2
                
                if valid_components and len(valid_components) >= min_components:
                    components_str = ",".join(valid_components)
                    component_count = len(valid_components)
                    
                    cursor.execute("""
                        INSERT OR REPLACE INTO radical_decomposition_mapping (radical, components, component_count)
                        VALUES (?, ?, ?)
                    """, (radical, components_str, component_count))
                    
                    valid_decompositions += 1
                    
                    # Track substitutions made
                    if len(expanded_components) != len(components):
                        substitution_count += 1
                    
                    # Debug specific examples
                    if radical in ['È≠ö', 'ËÇâ', 'È≥•', 'È¶¨', '‰∏∑'] or len(expanded_components) != len(components):
                        print(f"    üéØ Added decomposition: {radical} ‚Üí {valid_components} (from {components})")
        
        conn.commit()
        print(f"‚úÖ Added {valid_decompositions} radical decompositions from makemeahanzi")
        print(f"‚úÖ Added {corrected_count} manual corrections")
        print(f"üîÑ Applied substitutions/expansions to {substitution_count} radicals")

    def load_kanjidic_data(self, file_path: str) -> List[Dict]:
        """Load KanjiDic data from JSON file"""
        print(f"üìñ Loading KanjiDic data from {file_path}...")

        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Extract characters array from KanjiDic JSON structure
                characters = data.get('characters', [])
                print(f"‚úÖ Loaded {len(characters)} kanji entries from KanjiDic file")
                return characters
            except Exception as e:
                print(f"‚ùå Failed to load KanjiDic data from {file_path}: {e}")
                return []
        else:
            print(f"‚ö†Ô∏è  KanjiDic file not found at {file_path}")
            print("Please ensure you have kanjidic.json file in the assets directory")
            return []

    def populate_kanji_entries(self, conn: sqlite3.Connection, kanjidic_data: List[Dict], debug_mode: bool = False) -> None:
        """Populate the kanji_entries table"""
        cursor = conn.cursor()

        print(f"üìù Populating kanji entries... (debug_mode={debug_mode})")
        entries_added = 0

        for i, entry in enumerate(kanjidic_data):
            if i % 50 == 0 and i > 0:
                print(f"   Progress: {i}/{len(kanjidic_data)} kanji...")
                conn.commit()

            # KanjiDic2 format uses 'literal' for the kanji character
            kanji = entry.get('literal', entry.get('kanji', entry.get('character', '')))
            if not kanji:
                continue

            # Extract meanings from KanjiDic2 structure
            meanings = []
            reading_meaning = entry.get('readingMeaning', {})
            if 'groups' in reading_meaning:
                for group in reading_meaning['groups']:
                    if 'meanings' in group:
                        for meaning in group['meanings']:
                            if meaning.get('lang') == 'en':
                                meanings.append(meaning['value'])

            if not meanings and 'meanings' in entry: # Fallback for older formats
                meanings = entry['meanings']

            meanings_json = json.dumps(meanings, ensure_ascii=False)

            # Extract readings from KanjiDic2 structure
            kun_readings = []
            on_readings = []
            nanori_readings = []

            if 'groups' in reading_meaning:
                for group in reading_meaning['groups']:
                    if 'readings' in group:
                        for reading in group['readings']:
                            reading_type = reading.get('type', '')
                            reading_value = reading.get('value', '')

                            if reading_type == 'ja_kun':
                                kun_readings.append(reading_value)
                            elif reading_type == 'ja_on':
                                on_readings.append(reading_value)

            # Get nanori readings
            if 'nanori' in reading_meaning:
                nanori_readings = reading_meaning['nanori']

            # Fallback to old format (if 'groups' not used or empty)
            if not kun_readings and entry.get('kun_readings'):
                kun_readings = entry['kun_readings']
            if not on_readings and entry.get('on_readings'):
                on_readings = entry['on_readings']
            if not nanori_readings and entry.get('nanori_readings'):
                nanori_readings = entry['nanori_readings']

            kun_json = json.dumps(kun_readings if isinstance(kun_readings, list) else [kun_readings], ensure_ascii=False)
            on_json = json.dumps(on_readings if isinstance(on_readings, list) else [on_readings], ensure_ascii=False)
            nanori_json = json.dumps(nanori_readings if isinstance(nanori_readings, list) else [nanori_readings], ensure_ascii=False)

            # Extract metadata from KanjiDic2 structure
            misc = entry.get('misc', {})
            jlpt_level = misc.get('jlptLevel', entry.get('jlpt_level', entry.get('jlpt')))
            grade = misc.get('grade', entry.get('grade', entry.get('grade_level')))
            stroke_counts = misc.get('strokeCounts', [])
            stroke_count = stroke_counts[0] if stroke_counts else entry.get('stroke_count', entry.get('strokes'))
            frequency = misc.get('frequency', entry.get('frequency', entry.get('freq')))

            heisig_number = entry.get('heisig_number', entry.get('heisig'))
            heisig_keyword = entry.get('heisig_keyword', '')

            # Extract radical information from the radicals array
            radicals_list = entry.get('radicals', [])
            radical_number = None
            radical = ''
            
            # Find the classical radical
            for rad in radicals_list:
                if rad.get('type') == 'classical':
                    radical_number = rad.get('value')
                    break
            
            # Get radical name using a lookup table
            radical_names = []
            if radical_number:
                radical_name = get_radical_name(radical_number)
                if radical_name:
                    radical_names = [radical_name]
                    radical = radical_name
            
            radical_names_json = json.dumps(radical_names, ensure_ascii=False)

            components = entry.get('components', [])
            components_json = json.dumps(components if isinstance(components, list) else [components], ensure_ascii=False)

            try:
                cursor.execute("""
                    INSERT OR REPLACE INTO kanji_entries
                    (kanji, jlpt_level, grade, stroke_count, frequency,
                     meanings, kun_readings, on_readings, nanori_readings,
                     radical_names, heisig_number, heisig_keyword,
                     components, radical, radical_number)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (kanji, jlpt_level, grade, stroke_count, frequency,
                      meanings_json, kun_json, on_json, nanori_json,
                      radical_names_json, heisig_number, heisig_keyword,
                      components_json, radical, radical_number))

                entries_added += 1
                
                # In debug mode, commit after every successful insertion
                if debug_mode:
                    conn.commit()
                    if entries_added % 100 == 0:
                        print(f"   üêõ Debug: Successfully added {entries_added} kanji so far")
                        
            except sqlite3.Error as e:
                print(f"   ‚ùå Error inserting kanji '{kanji}': {e}")
                # Only suppress UNIQUE constraint errors, not other issues
                if "UNIQUE constraint failed" not in str(e):
                    print(f"   üîç Non-unique error for '{kanji}': {e}")
                    # Try to commit what we have so far to avoid losing progress
                    try:
                        conn.commit()
                        print(f"   üíæ Committed {entries_added} entries so far due to error")
                    except:
                        pass
                continue

        # Final commit
        conn.commit()
        print(f"‚úÖ Added {entries_added} kanji entries total")

    def load_jmnedict_data(self, file_path: str) -> List[Dict]:
        """Load JMnedict data from JSON file"""
        print(f"üìõ Loading JMnedict data from {file_path}...")

        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Handle new JMnedict format with metadata and words array
                if isinstance(data, dict) and 'words' in data:
                    words = data['words']
                    print(f"‚úÖ Loaded {len(words)} entries from JMnedict file (new format)")
                    return words
                elif isinstance(data, list):
                    # Old format - direct array of entries
                    print(f"‚úÖ Loaded {len(data)} entries from JMnedict file (old format)")
                    return data
                else:
                    print(f"‚ùå Unexpected JMnedict format")
                    return []
            except Exception as e:
                print(f"‚ùå Failed to load JMnedict data from {file_path}: {e}")
                return []
        else:
            print(f"‚ö†Ô∏è  JMnedict file not found at {file_path}")
            print("Please ensure you have jmnedict.json file in the assets directory")
            return []


    def rebuild_kanji_only(self) -> bool:
        """Rebuild only the kanji_entries table in existing database"""
        print(f"üîÑ Rebuilding kanji data in: {self.output_path}")
        
        if not os.path.exists(self.output_path):
            print(f"‚ùå Database not found: {self.output_path}")
            return False
        
        try:
            conn = sqlite3.connect(self.output_path)
            
            # Clear existing kanji data
            print("üóëÔ∏è  Clearing existing kanji data...")
            conn.execute("DELETE FROM kanji_entries")
            conn.commit()
            
            # Load KanjiDic data from single file
            print("üìñ Loading KanjiDic data...")
            kanjidic_file = "app/src/main/assets/kanjidic.json"
            kanjidic_data = self.load_kanjidic_data(kanjidic_file)
            
            if not kanjidic_data:
                print("‚ùå No KanjiDic data found!")
                conn.close()
                return False
            
            # Populate kanji entries
            print(f"üìù Populating {len(kanjidic_data)} kanji entries...")
            self.populate_kanji_entries(conn, kanjidic_data)
            
            # Commit and close
            conn.commit()
            conn.close()
            
            print("‚úÖ Kanji rebuild completed successfully!")
            return True
            
        except Exception as e:
            print(f"‚ùå Kanji rebuild failed: {e}")
            return False

    def build_database(self, jmdict_path: str = "app/src/main/assets/jmdict.json", kanjidic_path: str = "app/src/main/assets/kanjidic.json", kradfile_path: str = "app/src/main/assets/kradfile.json", radkfile_path: str = "app/src/main/assets/radkfile.json") -> bool:
        """Main method to build the complete database"""
        print(f"üöÄ Building database: {self.output_path}")
        print(f"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # Remove existing database
        if os.path.exists(self.output_path):
            print("üóëÔ∏è  Removing existing database...")
            os.remove(self.output_path)

        try:
            # Create new database
            conn = sqlite3.connect(self.output_path)
            conn.execute("PRAGMA journal_mode = WAL;") # Enable WAL for better concurrency
            conn.execute("PRAGMA synchronous = NORMAL;") # Optimize write performance

            # Create schema
            print("üìã Creating database schema...")
            self.create_database_schema(conn)

            # Populate tag definitions (JMdict POS tags and JMnedict types)
            print("üè∑Ô∏è  Populating tag definitions...")
            self.populate_tag_definitions(conn)

            # Populate pitch accent data
            print("üéµ Populating pitch accent data...")
            self.populate_pitch_accents(conn)
            
            # Populate word variants
            print("üîó Populating word variants...")
            self.populate_word_variants(conn)

            # Process KANJI and RADICAL data FIRST (faster to test)
            print("\n" + "="*50)
            print("üîß PROCESSING KANJI & RADICAL DATA (FAST)")
            print("="*50)
            
            # Load and populate kradfile data (kanji ‚Üí components)
            print("\nüîÑ Processing kradfile data...")
            kradfile_data = self.load_kradfile_data(kradfile_path)
            if kradfile_data:
                print(f"üìä Loaded {len(kradfile_data)} kanji entries from kradfile")
                self.populate_kanji_radical_mapping(conn, kradfile_data)
            else:
                print("‚ö†Ô∏è  No kradfile data found, skipping kanji radical mapping.")
            
            # Load radkfile data (for stroke counts and additional radicals)
            print("\nüîÑ Loading radkfile data...")
            radkfile_data = self.load_radkfile_data(radkfile_path)
            if radkfile_data:
                print(f"üìä Loaded {len(radkfile_data)} radicals from radkfile")
                
                # Show sample radical data for debugging
                sample_radicals = list(radkfile_data.keys())[:5]
                for radical in sample_radicals:
                    kanji_count = len(radkfile_data[radical].get('kanji', []))
                    stroke_count = radkfile_data[radical].get('strokeCount', 0)
                    print(f"  üîç {radical}: {kanji_count} kanji, {stroke_count} strokes")
                
                # Check specifically for ‰∫∫ radical
                if '‰∫∫' in radkfile_data:
                    person_kanji = radkfile_data['‰∫∫']['kanji']
                    has_fire = 'ÁÅ´' in person_kanji
                    print(f"  üéØ ‰∫∫ radical: {len(person_kanji)} kanji, contains ÁÅ´: {has_fire}")
                else:
                    print("  ‚ùå ‰∫∫ radical not found in radkfile!")
            else:
                print("‚ö†Ô∏è  No radkfile data found!")

            # Build radical ‚Üí kanji mapping from kradfile (primary) with radkfile support
            print("\nüîß Building radical ‚Üí kanji mapping...")
            if kradfile_data:
                self.populate_radical_kanji_mapping_from_kradfile(conn, kradfile_data, radkfile_data)
                
                # Verify the database entries
                print("\nüîç Verifying radical database entries...")
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM radical_kanji_mapping")
                radical_count = cursor.fetchone()[0]
                print(f"  üìä Total radicals in database: {radical_count}")
                
                # Check ‰∫∫ radical specifically
                cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = '‰∫∫'")
                person_result = cursor.fetchone()
                if person_result:
                    radical, strokes, kanji_list = person_result
                    kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                    has_fire = 'ÁÅ´' in kanji_list if kanji_list else False
                    print(f"  üéØ Database ‰∫∫ radical: {kanji_count} kanji, {strokes} strokes, contains ÁÅ´: {has_fire}")
                    if has_fire:
                        print("    ‚úÖ SUCCESS: ÁÅ´ found in ‰∫∫ radical!")
                    else:
                        print("    ‚ùå ISSUE: ÁÅ´ NOT found in ‰∫∫ radical")
                else:
                    print("  ‚ùå ‰∫∫ radical not found in database!")
                    
            elif radkfile_data:
                # Fallback to old method if only radkfile is available
                self.populate_radical_kanji_mapping(conn, radkfile_data)
            else:
                print("‚ö†Ô∏è  No radical data found, skipping radical kanji mapping.")
            
            # POST-PROCESSING: Override ALL radicals with enhanced radkfile data
            if radkfile_data:
                print("\nüîß POST-PROCESSING: Enhancing ALL radicals with radkfile data...")
                cursor = conn.cursor()
                
                updated_count = 0
                for radical, info in radkfile_data.items():
                    enhanced_kanji_list = info.get('kanji', [])
                    if enhanced_kanji_list:  # Only update if there's data
                        kanji_str = ", ".join(enhanced_kanji_list)
                        
                        # Debug ‰∫∫ radical specifically
                        if radical == '‰∫∫':
                            print(f"  üîç Updating ‰∫∫ radical with {len(enhanced_kanji_list)} kanji")
                            print(f"  üîç ÁÅ´ in list: {'ÁÅ´' in enhanced_kanji_list}")
                            print(f"  üîç First 10 kanji: {enhanced_kanji_list[:10]}")
                        
                        cursor.execute("""
                            UPDATE radical_kanji_mapping 
                            SET kanji_list = ?
                            WHERE radical = ?
                        """, (kanji_str, radical))
                        
                        if cursor.rowcount > 0:
                            updated_count += 1
                            if radical == '‰∫∫':
                                print(f"  ‚úÖ Successfully updated ‰∫∫ radical (rowcount={cursor.rowcount})")
                
                conn.commit()
                print(f"  ‚úÖ Updated {updated_count} radicals with enhanced data")
                
                # Verify ‰∫∫ radical specifically
                cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = '‰∫∫'")
                result = cursor.fetchone()
                if result:
                    radical, strokes, kanji_list = result
                    kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                    has_fire = 'ÁÅ´' in kanji_list if kanji_list else False
                    print(f"  üéØ ‰∫∫ radical after update: {kanji_count} kanji, contains ÁÅ´: {has_fire}")
                    if has_fire:
                        print("    ‚úÖ SUCCESS: ÁÅ´ found in ‰∫∫ radical!")
                
                print("  ‚úÖ Post-processing complete!")
                
                # Final verification before moving on
                print("\nüîç FINAL VERIFICATION of radical data...")
                cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = '‰∫∫'")
                final_result = cursor.fetchone()
                if final_result:
                    radical, strokes, kanji_list = final_result
                    kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                    has_fire = 'ÁÅ´' in kanji_list if kanji_list else False
                    print(f"  üéØ FINAL CHECK - ‰∫∫ radical: {kanji_count} kanji, contains ÁÅ´: {has_fire}")
                    
                    # Also check a few specific kanji
                    if kanji_list:
                        kanji_array = kanji_list.split(', ')
                        print(f"  üìù First 5 kanji: {kanji_array[:5]}")
                        if 'ÁÅ´' in kanji_array:
                            fire_index = kanji_array.index('ÁÅ´')
                            print(f"  üî• ÁÅ´ is at position {fire_index + 1}")
                else:
                    print("  ‚ùå ‰∫∫ radical not found in final check!")

            # RADICAL DECOMPOSITION: Load and populate makemeahanzi decomposition data
            print("\nüß© Processing radical decomposition data...")
            decomposition_data = self.load_makemeahanzi_decomposition_data()
            if decomposition_data:
                # Get existing radicals from the database to filter valid decompositions
                cursor.execute("SELECT radical FROM radical_kanji_mapping")
                existing_radicals = {row[0] for row in cursor.fetchall()}
                print(f"üìä Found {len(existing_radicals)} existing radicals in database")
                
                self.populate_radical_decomposition_mapping(conn, decomposition_data, existing_radicals)
                
                # Verify specific decomposition example
                cursor.execute("SELECT radical, components FROM radical_decomposition_mapping WHERE radical = '‰∏∑'")
                result = cursor.fetchone()
                if result:
                    radical, components = result
                    print(f"  üéØ Example decomposition: {radical} ‚Üí {components}")
                else:
                    print("  ‚ö†Ô∏è  No decomposition example found for ‰∏∑")
            else:
                print("‚ö†Ô∏è  No makemeahanzi decomposition data found, skipping radical decomposition.")

            # Load and populate KanjiDic data
            print("\nüìñ Processing KanjiDic data...")
            kanjidic_data = self.load_kanjidic_data(kanjidic_path)
            if kanjidic_data:
                print(f"üìä Loaded {len(kanjidic_data)} kanji from KanjiDic")
                self.populate_kanji_entries(conn, kanjidic_data)
            else:
                print("‚ö†Ô∏è  No KanjiDic data found, skipping KanjiDic entries.")

            print("\n" + "="*50)
            print("üéâ KANJI & RADICAL DATA COMPLETE!")
            print("You can test the radical search now or continue with full build...")
            print("="*50)
            
            # Process DICTIONARY data LAST (slower, for complete build)
            print("\nüìñ Processing JMdict data (this will take time)...")
            
            # Load JMdict data from single file
            jmdict_data = self.load_jmdict_data(jmdict_path)
            if not jmdict_data:
                print("üö® No JMdict data loaded. Cannot build dictionary.")
                return False

            print(f"üìä Loaded {len(jmdict_data)} entries from JMdict")

            # Load JMDict specific tags data
            tags_file_path = os.path.join(os.path.dirname(jmdict_path), "tags.json")
            tags_data_jmdict = self.load_tags_data(tags_file_path)

            # Populate dictionary data with JMdict entries
            print("üîÑ Populating JMdict entries...")
            self.populate_dictionary_entries(conn, jmdict_data, tags_data_jmdict)

            # Load and populate KanjiDic data from single file
            print("\nüàµ Processing KanjiDic data...")
            kanjidic_data = self.load_kanjidic_data(kanjidic_path)
            if kanjidic_data:
                self.populate_kanji_entries(conn, kanjidic_data)
            else:
                print("‚ö†Ô∏è  No KanjiDic data found, skipping kanji tables.")

            # NOTE: kradfile and radkfile processing already done at the beginning with post-processing

            # Populate FTS tables (after all data is inserted into main table)
            self.populate_fts_tables(conn)

            # Create triggers (after FTS tables are created)
            self.create_triggers(conn)

            # Verify database
            if not self.verify_database(conn):
                return False

            # Check ‰∫∫ radical before optimization
            print("\nüîç Checking ‰∫∫ radical BEFORE optimization...")
            cursor = conn.cursor()
            cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = '‰∫∫'")
            before_result = cursor.fetchone()
            if before_result:
                radical, strokes, kanji_list = before_result
                kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                has_fire = 'ÁÅ´' in kanji_list if kanji_list else False
                print(f"  üéØ BEFORE VACUUM - ‰∫∫ radical: {kanji_count} kanji, contains ÁÅ´: {has_fire}")
            
            # Optimize database
            print("\nüîß Optimizing database...")
            conn.execute("VACUUM")
            conn.execute("ANALYZE")
            
            # Check ‰∫∫ radical after optimization
            print("\nüîç Checking ‰∫∫ radical AFTER optimization...")
            cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = '‰∫∫'")
            after_result = cursor.fetchone()
            if after_result:
                radical, strokes, kanji_list = after_result
                kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                has_fire = 'ÁÅ´' in kanji_list if kanji_list else False
                print(f"  üéØ AFTER VACUUM - ‰∫∫ radical: {kanji_count} kanji, contains ÁÅ´: {has_fire}")

            conn.close()

            # Get final size
            size_mb = os.path.getsize(self.output_path) / (1024 * 1024)
            print(f"üìä Final database size: {size_mb:.2f} MB")
            print(f"‚úÖ Database built successfully: {self.output_path}")
            print(f"üìÖ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

            return True

        except Exception as e:
            print(f"‚ùå Database build failed: {e}")
            # print stack trace for debugging
            import traceback
            traceback.print_exc()
            return False

def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Build FTS5-enabled dictionary database")
    parser.add_argument("--jmdict", default="app/src/main/assets/jmdict.json",
                       help="Path to JMdict JSON file")
    parser.add_argument("--output", default="app/src/main/assets/databases/jmdict_fts5.db",
                       help="Output database path")
    parser.add_argument("--kanjidic", default="app/src/main/assets/kanjidic.json",
                       help="Path to KanjiDic JSON file")
    parser.add_argument("--kradfile", default="app/src/main/assets/kradfile.json",
                       help="Path to kradfile JSON file")
    parser.add_argument("--radkfile", default="app/src/main/assets/radkfile.json",
                       help="Path to radkfile JSON file")

    args = parser.parse_args()

    builder = DatabaseBuilder(args.output)
    success = builder.build_database(args.jmdict, args.kanjidic, args.kradfile, args.radkfile)

    if success:
        print("\nüéâ Database ready for Android deployment!")
        print(f"   Location: {args.output}")
        print("   Contains: JMdict entries + KanjiDic kanji data")
        print("   The app will automatically detect and use the pre-built database.")
    else:
        print("\n‚ùå Database build failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()