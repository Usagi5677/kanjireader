#!/usr/bin/env python3
"""
Unified Database Builder for Kanji Reader
Creates a complete FTS5-enabled database ready for Android app deployment

This script combines all the successful database building steps:
1. Creates main dictionary_entries table with FTS5 search
2. Creates kanji_entries table for KanjiDic data
3. Populates with JMdict and KanjiDic data
4. Tokenizes Japanese text properly for search
5. Creates all necessary indexes
6. Saves to app/src/main/assets/databases/
"""

import sqlite3
import json
import os
import sys
import shutil
from typing import List, Dict, Tuple, Optional
from datetime import datetime

# Try to import MeCab if available
try:
    import MeCab
    MECAB_AVAILABLE = True
except ImportError:
    MECAB_AVAILABLE = False
    print("âš ï¸  MeCab not available - will use basic tokenization")

def get_radical_name(radical_number: int) -> str:
    """
    Map Kangxi radical numbers to their Japanese names.
    This is a subset of the most common radicals.
    """
    radical_names = {
        1: "ä¸€", 2: "ä¸¨", 3: "ä¸¶", 4: "ä¸¿", 5: "ä¹™", 6: "äº…", 7: "äºŒ", 8: "äº ", 9: "äºº", 10: "å„¿",
        11: "å…¥", 12: "å…«", 13: "å†‚", 14: "å†–", 15: "å†«", 16: "å‡ ", 17: "å‡µ", 18: "åˆ€", 19: "åŠ›", 20: "å‹¹",
        21: "åŒ•", 22: "åŒš", 23: "åŒ¸", 24: "å", 25: "åœ", 26: "å©", 27: "åŽ‚", 28: "åŽ¶", 29: "åˆ", 30: "å£",
        31: "å›—", 32: "åœŸ", 33: "å£«", 34: "å¤‚", 35: "å¤Š", 36: "å¤•", 37: "å¤§", 38: "å¥³", 39: "å­", 40: "å®€",
        41: "å¯¸", 42: "å°", 43: "å°¢", 44: "å°¸", 45: "å±®", 46: "å±±", 47: "å·", 48: "å·¥", 49: "å·±", 50: "å·¾",
        51: "å¹²", 52: "å¹º", 53: "å¹¿", 54: "å»´", 55: "å»¾", 56: "å¼‹", 57: "å¼“", 58: "å½", 59: "å½¡", 60: "å½³",
        61: "å¿ƒ", 62: "æˆˆ", 63: "æˆ¸", 64: "æ‰‹", 65: "æ”¯", 66: "æ”´", 67: "æ–‡", 68: "æ–—", 69: "æ–¤", 70: "æ–¹",
        71: "æ— ", 72: "æ—¥", 73: "æ›°", 74: "æœˆ", 75: "æœ¨", 76: "æ¬ ", 77: "æ­¢", 78: "æ­¹", 79: "æ®³", 80: "æ¯‹",
        81: "æ¯”", 82: "æ¯›", 83: "æ°", 84: "æ°”", 85: "æ°´", 86: "ç«", 87: "çˆª", 88: "çˆ¶", 89: "çˆ»", 90: "çˆ¿",
        91: "ç‰‡", 92: "ç‰™", 93: "ç‰›", 94: "çŠ¬", 95: "çŽ„", 96: "çŽ‰", 97: "ç“œ", 98: "ç“¦", 99: "ç”˜", 100: "ç”Ÿ",
        101: "ç”¨", 102: "ç”°", 103: "ç–‹", 104: "ç–’", 105: "ç™¶", 106: "ç™½", 107: "çš®", 108: "çš¿", 109: "ç›®", 110: "çŸ›",
        111: "çŸ¢", 112: "çŸ³", 113: "ç¤º", 114: "ç¦¸", 115: "ç¦¾", 116: "ç©´", 117: "ç«‹", 118: "ç«¹", 119: "ç±³", 120: "ç³¸",
        121: "ç¼¶", 122: "ç½‘", 123: "ç¾Š", 124: "ç¾½", 125: "è€", 126: "è€Œ", 127: "è€’", 128: "è€³", 129: "è¿", 130: "è‚‰",
        131: "è‡£", 132: "è‡ª", 133: "è‡³", 134: "è‡¼", 135: "èˆŒ", 136: "èˆ›", 137: "èˆŸ", 138: "è‰®", 139: "è‰²", 140: "è‰¸",
        141: "è™", 142: "è™«", 143: "è¡€", 144: "è¡Œ", 145: "è¡£", 146: "è¥¾", 147: "è¦‹", 148: "è§’", 149: "è¨€", 150: "è°·",
        151: "è±†", 152: "è±•", 153: "è±¸", 154: "è²", 155: "èµ¤", 156: "èµ°", 157: "è¶³", 158: "èº«", 159: "è»Š", 160: "è¾›",
        161: "è¾°", 162: "è¾µ", 163: "é‚‘", 164: "é…‰", 165: "é‡†", 166: "é‡Œ", 167: "é‡‘", 168: "é•·", 169: "é–€", 170: "é˜œ",
        171: "éš¶", 172: "éš¹", 173: "é›¨", 174: "é’", 175: "éž", 176: "é¢", 177: "é©", 178: "éŸ‹", 179: "éŸ­", 180: "éŸ³",
        181: "é ", 182: "é¢¨", 183: "é£›", 184: "é£Ÿ", 185: "é¦–", 186: "é¦™", 187: "é¦¬", 188: "éª¨", 189: "é«˜", 190: "é«Ÿ",
        191: "é¬¥", 192: "é¬¯", 193: "é¬²", 194: "é¬¼", 195: "é­š", 196: "é³¥", 197: "é¹µ", 198: "é¹¿", 199: "éº¦", 200: "éº»",
        201: "é»„", 202: "é»", 203: "é»’", 204: "é»¹", 205: "é»½", 206: "é¼Ž", 207: "é¼“", 208: "é¼ ", 209: "é¼»", 210: "é½Š",
        211: "é½’", 212: "ç«œ", 213: "äº€", 214: "é¾ "
    }
    return radical_names.get(radical_number, "")

class DatabaseBuilder:
    def __init__(self, output_path: str = "app/src/main/assets/databases/jmdict_fts5.db"):
        self.output_path = output_path
        self.mecab_tokenizer = None
        self.frequency_data = {}
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Load frequency data
        self.load_frequency_data()
        
        # Initialize MeCab if available
        if MECAB_AVAILABLE:
            try:
                # IMPORTANT: For Windows, you might need to specify the path to mecabrc
                # For WSL/Linux, MeCab should usually be found if installed via package manager
                if sys.platform == "win32":
                    # Adjust this path if your MeCab installation is elsewhere
                    mecabrc_path = os.path.join(os.getenv('MECAB_PATH', r"C:\Program Files\MeCab"), "etc", "mecabrc")
                    if os.path.exists(mecabrc_path):
                        os.environ["MECABRC"] = mecabrc_path
                    else:
                        print(f"âš ï¸  MECABRC not found at {mecabrc_path}. MeCab might not initialize correctly.")
                self.mecab_tokenizer = MeCab.Tagger()
                print("âœ… MeCab tokenizer initialized")
            except Exception as e:
                print(f"âš ï¸  MeCab initialization failed: {e}")
                print("   Falling back to basic tokenization")

    def load_frequency_data(self) -> None:
        """Load frequency data from CSV file"""
        import csv
        frequency_file = "app/src/main/assets/frequency_data.csv"

        if not os.path.exists(frequency_file):
            print(f"âš ï¸  Frequency file not found: {frequency_file}")
            return

        try:
            with open(frequency_file, 'r', encoding='utf-8-sig') as f:
                # Skip BOM if present
                reader = csv.reader(f)
                next(reader)  # Skip header

                count = 0
                duplicates_found = 0
                skipped_corrupted = 0

                for row in reader:
                    if len(row) >= 4:
                        word = row[0].strip()
                        pos = row[1].strip() if len(row) > 1 else ""
                        reading = row[2].strip() if len(row) > 2 else ""
                        frequency_str = row[3].strip()

                        # Skip genuinely corrupted data (empty words or missing frequencies)
                        if not word or not frequency_str or frequency_str.strip() == '':
                            skipped_corrupted += 1
                            continue

                        # Parse frequency: remove commas and spaces, convert to int
                        try:
                            frequency = int(frequency_str.replace(',', '').replace(' ', ''))

                            # Create a unique key for true duplicates (same word, same POS, same reading)
                            unique_key = f"{word}|{pos}|{reading}"

                            # Keep the highest frequency for true duplicates
                            if unique_key in self.frequency_data:
                                existing_freq = self.frequency_data[unique_key]
                                if frequency > existing_freq:
                                    print(f"   ðŸ”„ Duplicate: '{word}' ({pos}|{reading}) - replacing freq {existing_freq:,} with {frequency:,}")
                                    self.frequency_data[unique_key] = frequency
                                    # Also update the simple word key for lookup
                                    if word not in self.frequency_data or frequency > self.frequency_data[word]:
                                        self.frequency_data[word] = frequency
                                else:
                                    print(f"   âŒ Duplicate: '{word}' ({pos}|{reading}) - keeping existing freq {existing_freq:,}, discarding {frequency:,}")
                                duplicates_found += 1
                            else:
                                self.frequency_data[unique_key] = frequency
                                # Also store by simple word key, keeping highest frequency
                                if word not in self.frequency_data or frequency > self.frequency_data[word]:
                                    self.frequency_data[word] = frequency
                                count += 1
                        except ValueError:
                            skipped_corrupted += 1
                            continue

                print(f"âœ… Loaded {count} frequency entries from CSV")
                if duplicates_found > 0:
                    print(f"âš ï¸  Found {duplicates_found} true duplicate entries - kept highest frequencies")
                if skipped_corrupted > 0:
                    print(f"âš ï¸  Skipped {skipped_corrupted} corrupted/empty entries")

        except Exception as e:
            print(f"âš ï¸  Error loading frequency data: {e}")

    def create_database_schema(self, conn: sqlite3.Connection) -> None:
        """Create all necessary tables and indexes"""
        cursor = conn.cursor()

        print("ðŸ“‹ Creating database schema...")

        # Main dictionary entries table
        # CHANGES: Removed 'jmnedict_type', added 'is_jmnedict_entry'
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS dictionary_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                kanji TEXT,
                reading TEXT NOT NULL,
                meanings TEXT NOT NULL,
                parts_of_speech TEXT,
                is_common INTEGER DEFAULT 0,
                frequency INTEGER DEFAULT 0,
                tokenized_kanji TEXT,
                tokenized_reading TEXT,
                is_jmnedict_entry INTEGER DEFAULT 0,  -- Only this flag for JMnedict source
                form_is_common INTEGER DEFAULT 0,  -- Common flag specific to this kanji-kana combination
                UNIQUE(kanji, reading, is_jmnedict_entry)
            )
        """)

        # Create indexes for performance
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_reading ON dictionary_entries(reading)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_kanji ON dictionary_entries(kanji)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_common ON dictionary_entries(is_common)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_frequency ON dictionary_entries(frequency)")
        # NEW INDEX for new column
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_is_jmnedict_entry ON dictionary_entries(is_jmnedict_entry)")

        # Create FTS5 virtual tables
        print("ðŸ“Š Creating FTS5 virtual tables...")

        # Japanese text search table (entries_fts5)
        # CHANGES: Removed jmnedict_type
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS entries_fts5 USING fts5(
                kanji,
                reading,
                meanings,
                tokenized_kanji,
                tokenized_reading,
                parts_of_speech,
                is_common UNINDEXED,
                frequency UNINDEXED,
                is_jmnedict_entry UNINDEXED, -- Only this flag for FTS
                form_is_common UNINDEXED,
                content='dictionary_entries',
                content_rowid='id'
            )
        """)

        # English search table
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS english_fts USING fts5(
                entry_id UNINDEXED,
                meanings,
                parts_of_speech,
                tokenize='unicode61',
                prefix='1 2 3'
            )
        """)

        # Tag definitions and word tags
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS tag_definitions (
                tag TEXT PRIMARY KEY,
                description TEXT
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS word_tags (
                entry_id INTEGER,
                tag TEXT,
                FOREIGN KEY(entry_id) REFERENCES dictionary_entries(id) ON DELETE CASCADE,
                FOREIGN KEY(tag) REFERENCES tag_definitions(tag) ON DELETE CASCADE
            )
        """)

        # Create indexes for tag tables
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_word_tags_entry ON word_tags(entry_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_word_tags_tag ON word_tags(tag)")

        # Pitch accent table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS pitch_accents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                kanji_form TEXT NOT NULL,
                reading TEXT NOT NULL,
                accent_pattern TEXT NOT NULL,
                UNIQUE(kanji_form, reading)
            )
        """)

        # Create indexes for pitch accent table
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_pitch_kanji ON pitch_accents(kanji_form)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_pitch_reading ON pitch_accents(reading)")

        print("âœ… Dictionary database schema created")

        # Create KanjiDic tables in the same database
        self.create_kanjidic_schema(conn)
        conn.commit()

    def populate_tag_definitions(self, conn: sqlite3.Connection) -> None:
        """Populate tag definitions table with JMdict POS tags"""
        cursor = conn.cursor()

        print("ðŸ·ï¸  Populating tag definitions...")

        # Complete tag definitions from jmdict.json
        tag_definitions = {
            # Basic parts of speech
            'n': 'noun (common) (futsuumeishi)',
            'n-adv': 'adverbial noun (fukushitekimeishi)', 
            'n-t': 'noun (temporal) (jisoumeishi)',
            'n-pr': 'proper noun',
            'n-pref': 'noun, used as a prefix',
            'n-suf': 'noun, used as a suffix',
            'adj-i': 'adjective (keiyoushi)',
            'adj-na': 'adjectival nouns or quasi-adjectives (keiyodoshi)',
            'adj-no': 'nouns which may take the genitive case particle \'no\'',
            'adj-pn': 'pre-noun adjectival (rentaishi)',
            'adj-t': '\'taru\' adjective',
            'adj-f': 'noun or verb acting prenominally',
            'adj-ix': 'adjective (keiyoushi) - yoi/ii class',
            'adj-ku': '\'ku\' adjective (archaic)',
            'adj-shiku': '\'shiku\' adjective (archaic)',
            'adj-nari': 'archaic/formal form of na-adjective',
            'adj-kari': '\'kari\' adjective (archaic)',
            'adv': 'adverb (fukushi)',
            'adv-to': 'adverb taking the \'to\' particle',
            'aux': 'auxiliary',
            'aux-v': 'auxiliary verb',
            'aux-adj': 'auxiliary adjective',
            'conj': 'conjunction',
            'cop': 'copula',
            'ctr': 'counter',
            'exp': 'expressions (phrases, clauses, etc.)',
            'id': 'idiomatic expression',
            'int': 'interjection (kandoushi)',
            'num': 'numeric',
            'pn': 'pronoun',
            'prt': 'particle',
            'pref': 'prefix',
            'suf': 'suffix',

            # Verbs - Regular
            'v1': 'Ichidan verb',
            'v1-s': 'Ichidan verb - kureru special class',
            'v5b': 'Godan verb with \'bu\' ending',
            'v5g': 'Godan verb with \'gu\' ending',
            'v5k': 'Godan verb with \'ku\' ending',
            'v5k-s': 'Godan verb - Iku/Yuku special class',
            'v5m': 'Godan verb with \'mu\' ending',
            'v5n': 'Godan verb with \'nu\' ending',
            'v5r': 'Godan verb with \'ru\' ending',
            'v5r-i': 'Godan verb with \'ru\' ending (irregular verb)',
            'v5s': 'Godan verb with \'su\' ending',
            'v5t': 'Godan verb with \'tsu\' ending',
            'v5u': 'Godan verb with \'u\' ending',
            'v5u-s': 'Godan verb with \'u\' ending (special class)',
            'v5uru': 'Godan verb - Uru old class verb (old form of Eru)',
            'v5aru': 'Godan verb - -aru special class',
            'vk': 'Kuru verb - special class',
            'vn': 'irregular nu verb',
            'vr': 'irregular ru verb, plain form ends with -ri',
            'vs': 'noun or participle which takes the aux. verb suru',
            'vs-c': 'su verb - precursor to the modern suru',
            'vs-s': 'suru verb - special class',
            'vs-i': 'suru verb - included',
            'vz': 'Ichidan verb - zuru verb (alternative form of -jiru verbs)',
            'vt': 'transitive verb',
            'vi': 'intransitive verb',
            'v-unspec': 'verb unspecified',

            # Archaic verbs
            'v2a-s': 'Nidan verb with \'u\' ending (archaic)',
            'v2b-k': 'Nidan verb (upper class) with \'bu\' ending (archaic)',
            'v2b-s': 'Nidan verb (lower class) with \'bu\' ending (archaic)',
            'v2d-k': 'Nidan verb (upper class) with \'dzu\' ending (archaic)',
            'v2d-s': 'Nidan verb (lower class) with \'dzu\' ending (archaic)',
            'v2g-k': 'Nidan verb (upper class) with \'gu\' ending (archaic)',
            'v2g-s': 'Nidan verb (lower class) with \'gu\' ending (archaic)',
            'v2h-k': 'Nidan verb (upper class) with \'hu/fu\' ending (archaic)',
            'v2h-s': 'Nidan verb (lower class) with \'hu/fu\' ending (archaic)',
            'v2k-k': 'Nidan verb (upper class) with \'ku\' ending (archaic)',
            'v2k-s': 'Nidan verb (lower class) with \'ku\' ending (archaic)',
            'v2m-k': 'Nidan verb (upper class) with \'mu\' ending (archaic)',
            'v2m-s': 'Nidan verb (lower class) with \'mu\' ending (archaic)',
            'v2n-s': 'Nidan verb (lower class) with \'nu\' ending (archaic)',
            'v2r-k': 'Nidan verb (upper class) with \'ru\' ending (archaic)',
            'v2r-s': 'Nidan verb (lower class) with \'ru\' ending (archaic)',
            'v2s-s': 'Nidan verb (lower class) with \'su\' ending (archaic)',
            'v2t-k': 'Nidan verb (upper class) with \'tsu\' ending (archaic)',
            'v2t-s': 'Nidan verb (lower class) with \'tsu\' ending (archaic)',
            'v2w-s': 'Nidan verb (lower class) with \'u\' ending and \'we\' conjugation (archaic)',
            'v2y-k': 'Nidan verb (upper class) with \'yu\' ending (archaic)',
            'v2y-s': 'Nidan verb (lower class) with \'yu\' ending (archaic)',
            'v2z-s': 'Nidan verb (lower class) with \'zu\' ending (archaic)',
            'v4b': 'Yodan verb with \'bu\' ending (archaic)',
            'v4g': 'Yodan verb with \'gu\' ending (archaic)',
            'v4h': 'Yodan verb with \'hu/fu\' ending (archaic)',
            'v4k': 'Yodan verb with \'ku\' ending (archaic)',
            'v4m': 'Yodan verb with \'mu\' ending (archaic)',
            'v4n': 'Yodan verb with \'nu\' ending (archaic)',
            'v4r': 'Yodan verb with \'ru\' ending (archaic)',
            'v4s': 'Yodan verb with \'su\' ending (archaic)',
            'v4t': 'Yodan verb with \'tsu\' ending (archaic)',

            # Usage and style tags
            'arch': 'archaic',
            'dated': 'dated term',
            'obs': 'obsolete term',
            'rare': 'rare term',
            'form': 'formal or literary term',
            'col': 'colloquial',
            'fam': 'familiar language',
            'sl': 'slang',
            'm-sl': 'manga slang',
            'net-sl': 'Internet slang',
            'hon': 'honorific or respectful (sonkeigo) language',
            'hum': 'humble (kenjougo) language',
            'pol': 'polite (teineigo) language',
            'male': 'male term or language',
            'fem': 'female term or language',
            'chn': 'children\'s language',
            'joc': 'jocular, humorous term',
            'vulg': 'vulgar expression or word',
            'derog': 'derogatory',
            'X': 'rude or X-rated term (not displayed in educational software)',
            'sens': 'sensitive',
            'euph': 'euphemistic',

            # Kanji and kana usage
            'sK': 'search-only kanji form',
            'rK': 'rarely used kanji form',
            'iK': 'word containing irregular kanji usage',
            'oK': 'word containing out-dated kanji or kanji usage',
            'sk': 'search-only kana form',
            'rk': 'rarely used kana form',
            'ik': 'word containing irregular kana usage',
            'ok': 'out-dated or obsolete kana usage',
            'uk': 'word usually written using kana alone',
            'io': 'irregular okurigana usage',
            'ateji': 'ateji (phonetic) reading',
            'gikun': 'gikun (meaning as reading) or jukujikun (special kanji reading)',

            # Specialized fields
            'abbr': 'abbreviation',
            'anat': 'anatomy',
            'agric': 'agriculture',
            'archeol': 'archeology',
            'archit': 'architecture',
            'art': 'art, aesthetics',
            'astron': 'astronomy',
            'audvid': 'audiovisual',
            'aviat': 'aviation',
            'baseb': 'baseball',
            'biochem': 'biochemistry',
            'biol': 'biology',
            'bot': 'botany',
            'boxing': 'boxing',
            'Buddh': 'Buddhism',
            'bus': 'business',
            'cards': 'card games',
            'chem': 'chemistry',
            'Christn': 'Christianity',
            'civeng': 'civil engineering',
            'cloth': 'clothing',
            'comp': 'computing',
            'cryst': 'crystallography',
            'dent': 'dentistry',
            'ecol': 'ecology',
            'econ': 'economics',
            'elec': 'electricity, elec. eng.',
            'electr': 'electronics',
            'embryo': 'embryology',
            'engr': 'engineering',
            'ent': 'entomology',
            'figskt': 'figure skating',
            'film': 'film',
            'finc': 'finance',
            'fish': 'fishing',
            'food': 'food, cooking',
            'gardn': 'gardening, horticulture',
            'genet': 'genetics',
            'geogr': 'geography',
            'geol': 'geology',
            'geom': 'geometry',
            'go': 'go (game)',
            'golf': 'golf',
            'gramm': 'grammar',
            'hanaf': 'hanafuda',
            'hist': 'historical term',
            'horse': 'horse racing',
            'internet': 'Internet',
            'kabuki': 'kabuki',
            'law': 'law',
            'ling': 'linguistics',
            'logic': 'logic',
            'MA': 'martial arts',
            'mahj': 'mahjong',
            'manga': 'manga',
            'math': 'mathematics',
            'mech': 'mechanical engineering',
            'med': 'medicine',
            'met': 'meteorology',
            'mil': 'military',
            'min': 'mineralogy',
            'mining': 'mining',
            'motor': 'motorsport',
            'music': 'music',
            'myth': 'mythology',
            'jpmyth': 'Japanese mythology',
            'grmyth': 'Greek mythology',
            'rommyth': 'Roman mythology',
            'chmyth': 'Chinese mythology',
            'noh': 'noh',
            'ornith': 'ornithology',
            'paleo': 'paleontology',
            'pathol': 'pathology',
            'pharm': 'pharmacology',
            'phil': 'philosophy',
            'photo': 'photography',
            'physics': 'physics',
            'physiol': 'physiology',
            'poet': 'poetical term',
            'politics': 'politics',
            'print': 'printing',
            'prowres': 'professional wrestling',
            'psych': 'psychology',
            'psyanal': 'psychoanalysis',
            'psy': 'psychiatry',
            'rail': 'railway',
            'relig': 'religion',
            'shogi': 'shogi',
            'Shinto': 'Shinto',
            'ski': 'skiing',
            'sports': 'sports',
            'stat': 'statistics',
            'stockm': 'stock market',
            'sumo': 'sumo',
            'surg': 'surgery',
            'telec': 'telecommunications',
            'tv': 'television',
            'vet': 'veterinary terms',
            'vidg': 'video games',
            'zool': 'zoology',

            # Mimetics and expressions
            'on-mim': 'onomatopoeic or mimetic word',
            'yoji': 'yojijukugo',
            'proverb': 'proverb',
            'quote': 'quotation',

            # Proper noun types
            'person': 'full name of a particular person',
            'given': 'given name or forename, gender not specified',
            'surname': 'family or surname',
            'place': 'place name',
            'station': 'railway station',
            'company': 'company name',
            'organization': 'organization name',
            'product': 'product name',
            'work': 'work of art, literature, music, etc. name',
            'char': 'character',
            'creat': 'creature',
            'dei': 'deity',
            'ev': 'event',
            'fict': 'fiction',
            'group': 'group',
            'leg': 'legend',
            'obj': 'object',
            'oth': 'other',
            'serv': 'service',
            'ship': 'ship name',
            'unc': 'unclassified',
            'unclass': 'unclassified name',
            'doc': 'document',
            'tradem': 'trademark',

            # Dialectal
            'hob': 'Hokkaido-ben',
            'thb': 'Touhoku-ben',
            'ktb': 'Kantou-ben',
            'kyb': 'Kyoto-ben',
            'ksb': 'Kansai-ben',
            'osb': 'Osaka-ben',
            'tsb': 'Tosa-ben',
            'tsug': 'Tsugaru-ben',
            'kyu': 'Kyuushuu-ben',
            'rkb': 'Ryuukyuu-ben',
            'nab': 'Nagano-ben',
            'bra': 'Brazilian',
            'masc': 'male name',
        }

        for tag, description in tag_definitions.items():
            cursor.execute("""
                INSERT OR REPLACE INTO tag_definitions (tag, description)
                VALUES (?, ?)
            """, (tag, description))

        conn.commit()
        print(f"âœ… Added {len(tag_definitions)} tag definitions")

    def populate_pitch_accents(self, conn: sqlite3.Connection) -> None:
        """Populate pitch accent table from accents.json"""
        cursor = conn.cursor()
        
        accents_file = os.path.join(os.path.dirname(__file__), 'app', 'src', 'main', 'assets', 'accents.json')
        
        if not os.path.exists(accents_file):
            print("âš ï¸  accents.json not found, skipping pitch accent data")
            return
            
        print("ðŸŽµ Populating pitch accent data...")
        
        try:
            with open(accents_file, 'r', encoding='utf-8') as f:
                accent_data = json.load(f)
            
            if 'accents' not in accent_data:
                print("âŒ Invalid accent data format")
                return
                
            count = 0
            skipped = 0
            
            for kanji_form, readings_dict in accent_data['accents'].items():
                for reading, accent_numbers in readings_dict.items():
                    try:
                        # Create accent pattern string (e.g., "1,2" for multiple patterns)
                        accent_pattern = ','.join(map(str, accent_numbers))
                        
                        cursor.execute("""
                            INSERT OR REPLACE INTO pitch_accents 
                            (kanji_form, reading, accent_pattern)
                            VALUES (?, ?, ?)
                        """, (kanji_form, reading, accent_pattern))
                        
                        count += 1
                        
                        if count % 10000 == 0:
                            print(f"   ðŸ“Š Processed {count:,} accent entries...")
                            
                    except Exception as e:
                        print(f"   âš ï¸  Skipping accent entry {kanji_form}:{reading} - {e}")
                        skipped += 1
                        continue
            
            conn.commit()
            print(f"âœ… Added {count:,} pitch accent entries")
            if skipped > 0:
                print(f"   âš ï¸  Skipped {skipped} invalid entries")
                
        except Exception as e:
            print(f"âŒ Error loading pitch accent data: {e}")

    def normalize_romaji(self, text: str) -> str:
        """
        Optional: Normalize Romaji characters like 'Å' (U+014D) to simpler ASCII 'o'.
        Add this function to fields that might contain such characters if desired.
        """
        if not text:
            return ""
        # Example: Replace macrons with their base ASCII equivalents
        text = text.replace('Ä', 'a').replace('Ä«', 'i').replace('Å«', 'u').replace('Ä“', 'e').replace('Å', 'o')
        text = text.replace('Ä€', 'A').replace('Äª', 'I').replace('Åª', 'U').replace('Ä’', 'E').replace('ÅŒ', 'O')
        # Add any other specific character replacements here if needed (e.g., apostrophes if they are mis-parsed)
        # text = text.replace("'", "") # Example: To remove straight apostrophes if undesired
        # text = text.replace("â€™", "") # Example: To remove curly apostrophes if undesired
        return text

    def tokenize_japanese(self, text: str) -> str:
        """
        Tokenize Japanese text for FTS5 search (removes spacing after tokenization)

        This function:
        1. Uses MeCab to properly tokenize compound words (e.g., "ã¿ã‚‹ã¹ã" -> "ã¿ã‚‹" + "ã¹ã")
        2. Removes spaces between tokens so FTS5 can find substring matches
        """
        if not text:
            return ""

        if self.mecab_tokenizer:
            try:
                node = self.mecab_tokenizer.parseToNode(text)
                tokens = []

                while node:
                    if node.surface:
                        tokens.append(node.surface)
                    node = node.next

                # IMPORTANT: Remove spaces after tokenization
                # Example: "ã¿ã‚‹ã¹ã" -> ["ã¿ã‚‹", "ã¹ã"] -> "ã¿ã‚‹ã¹ã" (no spaces)
                return "".join(tokens)
            except Exception as e:
                print(f"âš ï¸  MeCab tokenization failed for '{text}': {e}. Falling back to simple text.")

        # Fallback: basic character-level tokenization (no spaces needed as it's not tokenized)
        return text

    def calculate_frequency(self, word: str, is_common: bool, kana_reading: str = None, parts_of_speech: list = None) -> int:
        """
        Calculate frequency score for word.
        Assigns 0 frequency to known "pure proper noun" types to avoid false high rankings.
        """
        if parts_of_speech:
            # These are common JMnedict tags that indicate a proper name, not a dictionary word.
            pure_proper_noun_name_types = {
                'fem', 'masc', 'given', 'surname', 'person', 'char', 'place', 'station',
                'company', 'organization', 'group', 'product', 'serv', 'work', 'ev', 'obj',
                'creat', 'dei', 'myth', 'leg', 'ship', 'doc', 'relig', 'fict', 'oth', 'unclass'
            }

            # Check if any of the entry's POS/name tags match our pure proper noun types.
            if any(tag in pure_proper_noun_name_types for tag in parts_of_speech):
                return 0 # Assign 0 frequency to avoid contaminating general word frequencies.

        # Use exact matches from loaded frequency data.
        if word in self.frequency_data:
            return self.frequency_data[word]

        # No exact match found - return 0.
        return 0

    def load_jmdict_data(self, file_path: str) -> List[Dict]:
        """Load JMdict data from JSON file"""
        print(f"ðŸ“– Loading JMdict data from {file_path}...")

        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Handle new JMdict format with metadata and words array
                if isinstance(data, dict) and 'words' in data:
                    words = data['words']
                    print(f"âœ… Loaded {len(words)} entries from JMdict file (new format)")
                    return words
                elif isinstance(data, list):
                    # Old format - direct array of entries
                    print(f"âœ… Loaded {len(data)} entries from JMdict file (old format)")
                    return data
                else:
                    print(f"âŒ Unexpected JMdict format")
                    return []
            except Exception as e:
                print(f"âŒ Failed to load JMdict data from {file_path}: {e}")
                return []
        else:
            print(f"âŒ JMdict file not found at {file_path}")
            print("Please ensure you have jmdict.json file in the assets directory")
            return []

    def load_tags_data(self, tags_file_path: str) -> Dict[str, List[str]]:
        """Load tags data from tags.json file (JMdict POS tags)"""
        print(f"ðŸ·ï¸  Loading tags data from {tags_file_path}...")

        if not os.path.exists(tags_file_path):
            print(f"âš ï¸  No JMdict tags file found at {tags_file_path}")
            return {}

        try:
            with open(tags_file_path, 'r', encoding='utf-8') as f:
                tag_data = json.load(f)

            # Process the tags data (JMdict structure)
            all_tags = {}
            for word, tag_info in tag_data.items():
                if isinstance(tag_info, dict) and 'senses' in tag_info:
                    pos_tags = []
                    for sense in tag_info['senses']:
                        if 'pos' in sense:
                            pos_tags.extend(sense['pos'])

                    if pos_tags:
                        all_tags[word] = pos_tags

            print(f"âœ… Loaded tags for {len(all_tags)} words from JMdict tags file")
            return all_tags
        except Exception as e:
            print(f"âŒ Failed to load tags file: {e}")
            return {}

    # MODIFIED: populate_dictionary_entries now takes 'is_jmnedict_source'
    def populate_dictionary_entries(self, conn: sqlite3.Connection, entries_data: List[Dict], tags_data: Dict[str, List[str]], is_jmnedict_source: bool = False) -> None:
        """Populate the main dictionary_entries table and word_tags table"""
        cursor = conn.cursor()

        print(f"ðŸ“ Populating dictionary entries (Source: {'JMnedict' if is_jmnedict_source else 'JMdict'})...")
        entries_added = 0
        tags_added = 0
        error_count = 0  # Track SQL errors
        max_errors = 100  # Maximum allowed errors before stopping

        # NOTE: jmnedict_type_priority_order and related logic for jmnedict_specific_type are REMOVED.

        batch_size = 1000  # Smaller batch size to prevent corruption
        
        for i, entry in enumerate(entries_data):
            # Debug: Track ã¿ã‚‹ entries specifically
            kana_forms = [k['text'] for k in entry.get('kana', [])]
            if 'ã¿ã‚‹' in kana_forms:
                kanji_forms = [k['text'] for k in entry.get('kanji', [])]
                print(f"   ðŸ” Processing ã¿ã‚‹ entry #{i}: kanji={kanji_forms}, kana={kana_forms}")
            
            # Use smaller batches to prevent database corruption
            if i > 0 and i % batch_size == 0:
                try:
                    conn.commit()
                    # Perform integrity check periodically
                    if i % 10000 == 0:
                        print(f"   Progress: {i}/{len(entries_data)} entries...")
                        check_cursor = conn.execute("PRAGMA integrity_check")
                        result = check_cursor.fetchone()[0]
                        if result != "ok":
                            print(f"   âš ï¸  Database integrity issue detected: {result}")
                            raise sqlite3.DatabaseError("Database integrity check failed")
                except sqlite3.DatabaseError as e:
                    print(f"   âŒ Database error during commit: {e}")
                    if "database disk image is malformed" in str(e):
                        print("   ðŸš¨ CRITICAL: Database corruption detected. Stopping build.")
                        raise
                    # Try to recover by rolling back
                    try:
                        conn.rollback()
                        print("   ðŸ”„ Rolled back transaction, continuing...")
                    except:
                        raise

            # Handle new format with 'sense' array (JMdict) or 'translation' array (JMnedict)
            meanings = []
            parts_of_speech_list = []
            
            if 'sense' in entry:
                # JMdict format
                for sense in entry.get('sense', []):
                    # Extract meanings from gloss array
                    for gloss in sense.get('gloss', []):
                        if isinstance(gloss, dict):
                            # Standard JMdict format with lang field, or our custom format without lang
                            if gloss.get('lang') == 'eng' or not gloss.get('lang'):
                                text = gloss.get('text', '')
                                if text:
                                    meanings.append(text)
                    
                    # Extract parts of speech from each sense
                    # Support both 'partOfSpeech' (standard JMdict) and 'pos' (our custom entries)
                    pos_tags = sense.get('partOfSpeech', []) or sense.get('pos', [])
                    parts_of_speech_list.extend(pos_tags)
            elif 'translation' in entry:
                # JMnedict format
                for trans in entry.get('translation', []):
                    # Extract meanings from translation array
                    for t in trans.get('translation', []):
                        if isinstance(t, dict) and t.get('lang') == 'eng':
                            text = t.get('text', '')
                            if text:
                                meanings.append(text)
                    
                    # Extract name types as parts of speech
                    name_types = trans.get('type', [])
                    parts_of_speech_list.extend(name_types)
                    
                    # Debug logging for JMnedict tag extraction
                    if is_jmnedict_source:
                        kanji_forms_debug = [k['text'] for k in entry.get('kanji', [])]
                        if 'ä¸Š' in kanji_forms_debug:
                            print(f"   ðŸ” JMnedict tag extraction for ä¸Š: name_types={name_types}")
            else:
                # Old format fallback
                meanings = entry.get('meanings', [])
            
            if not meanings:
                continue

            meanings_json = json.dumps(meanings)

            kanji_forms = [k['text'] for k in entry.get('kanji', [])]
            kana_forms = [k['text'] for k in entry.get('kana', [])]

            # Collect additional POS tags from tags file (if not already populated from sense)
            if not parts_of_speech_list:
                for word in kanji_forms + kana_forms:
                    if word in tags_data:
                        parts_of_speech_list.extend(tags_data[word])
            
            # Remove duplicates, preserve order
            parts_of_speech_list = list(dict.fromkeys(parts_of_speech_list))
            parts_of_speech_json = json.dumps(parts_of_speech_list)

            is_common = any(form.get('common', False) for form in entry.get('kanji', []) + entry.get('kana', []))

            # --- UPDATED LOGIC FOR FLAGS ---
            is_entry_from_jmnedict = 1 if is_jmnedict_source else 0
            # jmnedict_specific_type is REMOVED from here.

            # Build forms with their metadata
            forms_to_insert = []
            kanji_entries = entry.get('kanji', [])
            kana_entries = entry.get('kana', [])
            
            if kanji_forms:
                for kanji_text in kanji_forms:
                    # Find the kanji entry object for this text
                    kanji_entry = next((k for k in kanji_entries if k['text'] == kanji_text), {})
                    kanji_common = kanji_entry.get('common', False)
                    
                    for kana_text in kana_forms:
                        # Find the kana entry object for this text
                        kana_entry = next((k for k in kana_entries if k['text'] == kana_text), {})
                        kana_common = kana_entry.get('common', False)
                        
                        # Form is common if both kanji and kana are common
                        form_common = kanji_common and kana_common
                        
                        forms_to_insert.append((kanji_text, kana_text, form_common))
            elif kana_forms:
                for kana_text in kana_forms:
                    # Find the kana entry object for this text
                    kana_entry = next((k for k in kana_entries if k['text'] == kana_text), {})
                    kana_common = kana_entry.get('common', False)
                    
                    forms_to_insert.append((None, kana_text, kana_common))

            for kanji_form, kana_form, form_is_common in forms_to_insert:
                # OPTIONAL: Apply Romaji normalization here if desired
                # normalized_kana_form = self.normalize_romaji(kana_form)
                # normalized_kanji_form = self.normalize_romaji(kanji_form) if kanji_form else None

                tokenized_kanji = self.tokenize_japanese(kanji_form) if kanji_form else None
                tokenized_reading = self.tokenize_japanese(kana_form)

                frequency = self.calculate_frequency(kanji_form if kanji_form else kana_form, is_common, kana_form, parts_of_speech_list)

                try:
                    # Both JMdict and JMNEdict entries should be inserted - they serve different purposes
                    # JMdict = dictionary words, JMNEdict = proper nouns/names
                    # The UNIQUE constraint is on (kanji, reading) but we want both types when they exist
                    
                    # Debug logging for ã¿ã‚‹ entries
                    if kana_form == 'ã¿ã‚‹':
                        print(f"   ðŸ” DEBUG: Inserting ã¿ã‚‹ entry:")
                        print(f"      kanji_form: {repr(kanji_form)}")
                        print(f"      kana_form: {repr(kana_form)}")
                        print(f"      frequency: {frequency}")
                        print(f"      is_common: {is_common}")
                        print(f"      is_entry_from_jmnedict: {is_entry_from_jmnedict}")
                        print(f"      parts_of_speech: {parts_of_speech_list}")
                    
                    cursor.execute("""
                        INSERT OR IGNORE INTO dictionary_entries
                        (kanji, reading, meanings, parts_of_speech, is_common,
                         frequency, tokenized_kanji, tokenized_reading,
                         is_jmnedict_entry, form_is_common)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (kanji_form, kana_form, meanings_json, parts_of_speech_json,
                          1 if is_common else 0, frequency, tokenized_kanji, tokenized_reading,
                          is_entry_from_jmnedict, 1 if form_is_common else 0))

                    # Get the entry ID - check if entry was actually inserted
                    if cursor.rowcount > 0:
                        # Entry was inserted successfully
                        entry_id = cursor.lastrowid
                    else:
                        # INSERT was ignored (duplicate), fetch the existing entry's ID
                        cursor.execute("""
                            SELECT id FROM dictionary_entries 
                            WHERE reading = ? AND (kanji IS ? OR (kanji IS NULL AND ? IS NULL)) AND is_jmnedict_entry = ?
                        """, (kana_form, kanji_form, kanji_form, is_entry_from_jmnedict))
                        result = cursor.fetchone()
                        entry_id = result[0] if result else None
                    
                    # Debug logging for ã¿ã‚‹ entries - check if insertion was successful
                    if kana_form == 'ã¿ã‚‹':
                        if entry_id:
                            print(f"   âœ… DEBUG: ã¿ã‚‹ entry inserted successfully with ID: {entry_id}")
                        else:
                            print(f"   âŒ DEBUG: ã¿ã‚‹ entry insertion was ignored (likely duplicate)")
                            # Check what existing entry conflicts
                            cursor.execute("""
                                SELECT id, kanji, reading, frequency, is_common, is_jmnedict_entry 
                                FROM dictionary_entries 
                                WHERE reading = ? AND (kanji IS ? OR (kanji IS NULL AND ? IS NULL)) AND is_jmnedict_entry = ?
                            """, (kana_form, kanji_form, kanji_form, is_entry_from_jmnedict))
                            existing = cursor.fetchone()
                            if existing:
                                print(f"      Existing entry: ID={existing[0]}, kanji={repr(existing[1])}, freq={existing[3]}, is_jmnedict={existing[5]}")
                            else:
                                print(f"      No conflicting entry found - insertion failed for other reason")

                    # Debug logging for JMnedict entries
                    if is_entry_from_jmnedict and kanji_form == 'ä¸Š':
                        print(f"   ðŸ” DEBUG JMnedict: {kanji_form}/{kana_form}")
                        print(f"      entry_id: {entry_id}")
                        print(f"      parts_of_speech_list: {parts_of_speech_list}")
                        print(f"      tags to insert: {len(parts_of_speech_list)}")

                    if entry_id:
                        for tag in parts_of_speech_list:
                            try:
                                cursor.execute("""
                                    INSERT OR REPLACE INTO word_tags (entry_id, tag)
                                    VALUES (?, ?)
                                """, (entry_id, tag))
                                tags_added += 1
                                
                                # Debug logging for JMnedict entries
                                if is_entry_from_jmnedict and kanji_form == 'ä¸Š':
                                    print(f"         âœ… Inserted tag: {tag}")
                            except sqlite3.Error as e:
                                if is_entry_from_jmnedict and kanji_form == 'ä¸Š':
                                    print(f"         âŒ Failed to insert tag: {tag}, error: {e}")
                                pass
                    else:
                        if is_entry_from_jmnedict and kanji_form == 'ä¸Š':
                            print(f"   âŒ No valid entry_id for {kanji_form}/{kana_form}")
                    entries_added += 1
                except sqlite3.Error as e:
                    error_count += 1
                    print(f"   âš ï¸  SQL Error inserting {kanji_form}/{kana_form}: {e}")
                    if "database disk image is malformed" in str(e):
                        print(f"   ðŸš¨ CRITICAL: Database corruption detected after {i} entries")
                        raise
                    if error_count > max_errors:
                        print(f"   ðŸš¨ Too many errors ({error_count}), stopping build")
                        raise RuntimeError(f"Too many SQL errors: {error_count}")
                    continue

        # Final commit
        try:
            conn.commit()
            print(f"âœ… Added {entries_added} dictionary entries from this source")
            print(f"âœ… Added {tags_added} word tags from this source")
            if error_count > 0:
                print(f"   âš ï¸  Encountered {error_count} errors during insertion")
        except sqlite3.DatabaseError as e:
            print(f"   âŒ Final commit failed: {e}")
            raise

    def populate_fts_tables(self, conn: sqlite3.Connection) -> None:
        """Populate FTS5 virtual tables"""
        cursor = conn.cursor()

        print("ðŸ” Populating FTS5 tables...")

        # Populate entries_fts5 table
        # This will automatically pull the new columns (is_common, frequency, is_jmnedict_entry)
        # because they are specified in the CREATE VIRTUAL TABLE DDL.
        # FTS5 'rebuild' command ensures all data from the content table is re-indexed.
        cursor.execute("""
            INSERT INTO entries_fts5(entries_fts5) VALUES('rebuild')
        """)

        # Populate english_fts table (no changes needed here as it doesn't use the new columns)
        cursor.execute("""
            INSERT INTO english_fts (entry_id, meanings, parts_of_speech)
            SELECT id, meanings, parts_of_speech FROM dictionary_entries
        """)

        conn.commit()
        print("âœ… FTS5 tables populated")

    def create_triggers(self, conn: sqlite3.Connection) -> None:
        """Create triggers to keep FTS5 tables in sync"""
        cursor = conn.cursor()

        print("ðŸ”§ Creating triggers...")

        # Trigger for entries_fts5 (AFTER INSERT)
        # CHANGES: Removed jmnedict_type from VALUES list
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS entries_fts5_ai AFTER INSERT ON dictionary_entries
            BEGIN
                INSERT INTO entries_fts5(rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency, is_jmnedict_entry, form_is_common)
                VALUES (new.id, new.kanji, new.reading, new.meanings, new.tokenized_kanji,
                        new.tokenized_reading, new.parts_of_speech,
                        new.is_common, new.frequency, new.is_jmnedict_entry, new.form_is_common);
            END
        """)

        # Trigger for entries_fts5 (AFTER DELETE)
        # CHANGES: Removed jmnedict_type from VALUES list
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS entries_fts5_ad AFTER DELETE ON dictionary_entries
            BEGIN
                INSERT INTO entries_fts5(entries_fts5, rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency, is_jmnedict_entry, form_is_common)
                VALUES ('delete', old.id, old.kanji, old.reading, old.meanings, old.tokenized_kanji,
                        old.tokenized_reading, old.parts_of_speech,
                        old.is_common, old.frequency, old.is_jmnedict_entry, old.form_is_common);
            END
        """)

        # Trigger for entries_fts5 (AFTER UPDATE)
        # CHANGES: Removed jmnedict_type from VALUES list in both delete and insert parts
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS entries_fts5_au AFTER UPDATE ON dictionary_entries
            BEGIN
                INSERT INTO entries_fts5(entries_fts5, rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency, is_jmnedict_entry, form_is_common)
                VALUES ('delete', old.id, old.kanji, old.reading, old.meanings, old.tokenized_kanji,
                        old.tokenized_reading, old.parts_of_speech,
                        old.is_common, old.frequency, old.is_jmnedict_entry, old.form_is_common);
                INSERT INTO entries_fts5(rowid, kanji, reading, meanings, tokenized_kanji,
                                        tokenized_reading, parts_of_speech,
                                        is_common, frequency, is_jmnedict_entry, form_is_common)
                VALUES (new.id, new.kanji, new.reading, new.meanings, new.tokenized_kanji,
                        new.tokenized_reading, new.parts_of_speech,
                        new.is_common, new.frequency, new.is_jmnedict_entry, new.form_is_common);
            END
        """)

        # english_fts triggers remain the same as they don't use these new columns
        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS english_fts_ai AFTER INSERT ON dictionary_entries
            BEGIN
                INSERT INTO english_fts(entry_id, meanings, parts_of_speech)
                VALUES (new.id, new.meanings, new.parts_of_speech);
            END
        """)

        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS english_fts_ad AFTER DELETE ON dictionary_entries
            BEGIN
                DELETE FROM english_fts WHERE entry_id = old.id;
            END
        """)

        cursor.execute("""
            CREATE TRIGGER IF NOT EXISTS english_fts_au AFTER UPDATE ON dictionary_entries
            BEGIN
                UPDATE english_fts SET meanings = new.meanings, parts_of_speech = new.parts_of_speech
                WHERE entry_id = old.id;
            END
        """)

        print("âœ… Triggers created")

    def verify_database(self, conn: sqlite3.Connection) -> bool:
        """Verify the database was created correctly"""
        cursor = conn.cursor()

        print("ðŸ” Verifying database...")

        # Check table counts
        cursor.execute("SELECT COUNT(*) FROM dictionary_entries")
        entry_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM entries_fts5")
        fts5_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM english_fts")
        english_fts_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM tag_definitions")
        tag_defs_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM word_tags")
        word_tags_count = cursor.fetchone()[0]

        # Check kanji tables too
        cursor.execute("SELECT COUNT(*) FROM kanji_entries")
        kanji_count = cursor.fetchone()[0]

        print(f"   Dictionary entries: {entry_count}")
        print(f"   FTS5 entries: {fts5_count}")
        print(f"   English FTS entries: {english_fts_count}")
        print(f"   Tag definitions: {tag_defs_count}")
        print(f"   Word tags: {word_tags_count}")
        print(f"   Kanji entries: {kanji_count}")

        # Test search functionality
        cursor.execute("""
            SELECT COUNT(*) FROM dictionary_entries
            WHERE id IN (
                SELECT rowid FROM entries_fts5
                WHERE reading MATCH ? OR tokenized_reading MATCH ?
            )
        """, ('ã¿ã‚‹', 'ã¿ã‚‹'))

        search_results = cursor.fetchone()[0]
        print(f"   Search test (ã¿ã‚‹): {search_results} results")

        # Test tag functionality
        cursor.execute("""
            SELECT COUNT(*) FROM word_tags wt
            JOIN tag_definitions td ON wt.tag = td.tag
            WHERE wt.tag = 'v1'
        """)
        tag_test = cursor.fetchone()[0]
        print(f"   Tag test (v1 verbs): {tag_test} results")

        # Test kanji lookup
        cursor.execute("""
            SELECT COUNT(*) FROM kanji_entries
            WHERE kanji = 'æ°´'
        """)
        kanji_test = cursor.fetchone()[0]
        print(f"   Kanji test ('æ°´'): {kanji_test} results")

        # Check if we have reasonable data (FIXED: tag_defs_count >= 50)
        if (entry_count > 100000 and fts5_count == entry_count and
            search_results > 0 and tag_defs_count >= 50 and word_tags_count > 100000):
            print("âœ… Database verification passed")
            return True
        else:
            print("âŒ Database verification failed")
            return False

    def create_kanjidic_schema(self, conn: sqlite3.Connection) -> None:
        """Create schema for KanjiDic database"""
        cursor = conn.cursor()

        print("ðŸ“‹ Creating KanjiDic database schema...")

        # Main kanji entries table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS kanji_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                kanji TEXT NOT NULL UNIQUE,
                jlpt_level INTEGER,
                grade INTEGER,
                stroke_count INTEGER,
                frequency INTEGER,
                meanings TEXT,
                kun_readings TEXT,
                on_readings TEXT,
                nanori_readings TEXT,
                radical_names TEXT,
                heisig_number INTEGER,
                heisig_keyword TEXT,
                components TEXT,
                radical TEXT,
                radical_number INTEGER
            )
        """)

        # Create indexes
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_kanji ON kanji_entries(kanji)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_jlpt ON kanji_entries(jlpt_level)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_grade ON kanji_entries(grade)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_frequency ON kanji_entries(frequency)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_stroke_count ON kanji_entries(stroke_count)")

        # Kanji radical mapping tables (from kradfile.json)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS kanji_radical_mapping (
                kanji TEXT PRIMARY KEY,
                components TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS radical_kanji_mapping (
                radical TEXT PRIMARY KEY,
                stroke_count INTEGER,
                kanji_list TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS radical_decomposition_mapping (
                radical TEXT PRIMARY KEY,
                components TEXT NOT NULL,
                component_count INTEGER NOT NULL
            )
        """)

        # Create indexes for radical tables
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_radical_stroke_count ON radical_kanji_mapping(stroke_count)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_decomposition_component_count ON radical_decomposition_mapping(component_count)")

        # No FTS5 tables needed for kanji - direct lookup is sufficient

        print("âœ… KanjiDic database schema created")

    def load_kradfile_data(self, file_path: str) -> Optional[Dict]:
        """Load kradfile data from JSON file"""
        print(f"ðŸ“– Loading kradfile data from {file_path}...")
        
        if not os.path.exists(file_path):
            print(f"âš ï¸  kradfile not found: {file_path}")
            return None
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            kanji_data = data.get('kanji', {})
            print(f"âœ… Loaded {len(kanji_data)} kanji entries from kradfile")
            return kanji_data
            
        except Exception as e:
            print(f"ðŸš¨ Error loading kradfile data: {e}")
            return None

    def load_radkfile_data(self, file_path: str) -> Optional[Dict]:
        """Load radkfile data from JSON file"""
        print(f"ðŸ“– Loading radkfile data from {file_path}...")
        
        if not os.path.exists(file_path):
            print(f"âš ï¸  radkfile not found: {file_path}")
            return None
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract the radicals data
            radicals_data = data.get('radicals', {})
            print(f"âœ… Loaded {len(radicals_data)} radical entries from radkfile")
            return radicals_data
            
        except Exception as e:
            print(f"ðŸš¨ Error loading radkfile data: {e}")
            return None

    def populate_kanji_radical_mapping(self, conn: sqlite3.Connection, kradfile_data: Dict) -> None:
        """Populate kanji_radical_mapping table from kradfile data"""
        cursor = conn.cursor()
        
        print("ðŸ”§ Populating kanji radical mapping...")
        
        for kanji, components in kradfile_data.items():
            # Convert list of components to comma-separated string
            components_str = ", ".join(components) if isinstance(components, list) else str(components)
            
            cursor.execute("""
                INSERT OR REPLACE INTO kanji_radical_mapping (kanji, components)
                VALUES (?, ?)
            """, (kanji, components_str))
        
        conn.commit()
        print(f"âœ… Added {len(kradfile_data)} kanji component mappings")

    def populate_radical_kanji_mapping_from_kradfile(self, conn: sqlite3.Connection, kradfile_data: Dict, radkfile_data: Dict = None) -> None:
        """Build radical_kanji_mapping table from kradfile data (primary) with radkfile fallback"""
        cursor = conn.cursor()
        
        print("ðŸ”§ Building radical kanji mapping from kradfile data...")
        
        # Build radical -> kanji mapping from kradfile
        radical_to_kanji = {}
        
        print("  ðŸ“ Inverting kradfile (kanji â†’ components) to (radical â†’ kanji list)...")
        for kanji, components in kradfile_data.items():
            if isinstance(components, list):
                for radical in components:
                    if radical not in radical_to_kanji:
                        radical_to_kanji[radical] = []
                    radical_to_kanji[radical].append(kanji)
                    
                    # Debug äºº radical specifically
                    if radical == 'äºº' and kanji == 'ç«':
                        print(f"    ðŸŽ¯ Found ç« with äºº component in kradfile!")
        
        print(f"  ðŸ“Š Built mapping for {len(radical_to_kanji)} unique radicals from kradfile")
        
        # Check äºº radical from kradfile inversion
        if 'äºº' in radical_to_kanji:
            person_kanji_from_krad = radical_to_kanji['äºº']
            has_fire = 'ç«' in person_kanji_from_krad
            print(f"  ðŸŽ¯ äºº radical from kradfile inversion: {len(person_kanji_from_krad)} kanji, contains ç«: {has_fire}")
        else:
            print("  âŒ äºº radical not found in kradfile inversion!")
        
        # Get stroke counts from radkfile if available
        radical_stroke_counts = {}
        if radkfile_data:
            print("  ðŸ“ Getting stroke counts from radkfile...")
            for radical, info in radkfile_data.items():
                stroke_count = info.get('strokeCount', 0)
                radical_stroke_counts[radical] = stroke_count
        
        # Insert or update radical mappings
        for radical, kanji_list in radical_to_kanji.items():
            # Get stroke count from radkfile, default to 1 if not found
            stroke_count = radical_stroke_counts.get(radical, 1)
            
            # Convert list of kanji to comma-separated string
            kanji_str = ", ".join(sorted(kanji_list))  # Sort for consistency
            
            cursor.execute("""
                INSERT OR REPLACE INTO radical_kanji_mapping (radical, stroke_count, kanji_list)
                VALUES (?, ?, ?)
            """, (radical, stroke_count, kanji_str))
        
        conn.commit()
        print(f"âœ… Built radical mappings for {len(radical_to_kanji)} radicals from kradfile data")
        
        # Add any radicals from radkfile that weren't in kradfile
        if radkfile_data:
            radkfile_only_count = 0
            for radical, info in radkfile_data.items():
                if radical not in radical_to_kanji:
                    stroke_count = info.get('strokeCount', 0)
                    kanji_list = info.get('kanji', [])
                    kanji_str = ", ".join(kanji_list) if isinstance(kanji_list, list) else str(kanji_list)
                    
                    cursor.execute("""
                        INSERT OR REPLACE INTO radical_kanji_mapping (radical, stroke_count, kanji_list)
                        VALUES (?, ?, ?)
                    """, (radical, stroke_count, kanji_str))
                    radkfile_only_count += 1
            
            if radkfile_only_count > 0:
                conn.commit()
                print(f"âœ… Added {radkfile_only_count} additional radicals from radkfile")

    def populate_radical_kanji_mapping(self, conn: sqlite3.Connection, radkfile_data: Dict) -> None:
        """Populate radical_kanji_mapping table from radkfile data (legacy method)"""
        cursor = conn.cursor()
        
        print("ðŸ”§ Populating radical kanji mapping...")
        
        for radical, info in radkfile_data.items():
            stroke_count = info.get('strokeCount', 0)
            kanji_list = info.get('kanji', [])
            
            # Convert list of kanji to comma-separated string
            kanji_str = ", ".join(kanji_list) if isinstance(kanji_list, list) else str(kanji_list)
            
            cursor.execute("""
                INSERT OR REPLACE INTO radical_kanji_mapping (radical, stroke_count, kanji_list)
                VALUES (?, ?, ?)
            """, (radical, stroke_count, kanji_str))
        
        conn.commit()
        print(f"âœ… Added {len(radkfile_data)} radical kanji mappings")

    def parse_makemeahanzi_decomposition(self, decomposition: str) -> List[str]:
        """
        Parse makemeahanzi decomposition field, extracting component radicals
        
        Args:
            decomposition: The decomposition string like "â¿»äº…å…«" or "â¿°æ°µé’"
            
        Returns:
            List of component radicals (without IDC symbols)
        """
        if not decomposition:
            return []

        # IDC (Ideographic Description Characters) to ignore
        idc_chars = {
            'â¿°', 'â¿±', 'â¿²', 'â¿³', 'â¿´', 'â¿µ', 'â¿¶', 'â¿·', 'â¿¸', 'â¿¹', 'â¿º', 'â¿»'
        }

        # Extract all characters except IDC symbols
        components = []
        for char in decomposition:
            if char not in idc_chars and char.strip():
                components.append(char)

        return components

    def load_makemeahanzi_decomposition_data(self, makemeahanzi_path: str = None) -> Dict[str, List[str]]:
        """
        Load makemeahanzi decomposition data from downloaded dictionary.txt
        
        Args:
            makemeahanzi_path: Optional path to makemeahanzi file, if None will try to download
            
        Returns:
            Dictionary mapping radicals to their component radicals
        """
        print("ðŸ“– Loading makemeahanzi decomposition data...")
        
        # If no path provided, try to download makemeahanzi data
        temp_dir = None
        if not makemeahanzi_path:
            try:
                # Try to use the same download logic from preserve_modifications.py
                import tempfile
                import urllib.request
                import zipfile
                
                temp_dir = tempfile.mkdtemp()
                makemeahanzi_zip = os.path.join(temp_dir, "makemeahanzi.zip")
                makemeahanzi_path = os.path.join(temp_dir, "dictionary.txt")
                
                print("â¬‡ï¸  Downloading makemeahanzi data...")
                urllib.request.urlretrieve(
                    "https://github.com/skishore/makemeahanzi/archive/refs/heads/master.zip",
                    makemeahanzi_zip
                )
                
                with zipfile.ZipFile(makemeahanzi_zip, 'r') as zip_ref:
                    zip_ref.extract("makemeahanzi-master/dictionary.txt", temp_dir)
                    
                # Move the extracted file to the expected location
                extracted_path = os.path.join(temp_dir, "makemeahanzi-master", "dictionary.txt")
                if os.path.exists(extracted_path):
                    shutil.move(extracted_path, makemeahanzi_path)
                
                print(f"âœ… Downloaded makemeahanzi to {makemeahanzi_path}")
                
            except Exception as e:
                print(f"âš ï¸  Could not download makemeahanzi data: {e}")
                return {}
        
        if not os.path.exists(makemeahanzi_path):
            print(f"âš ï¸  Makemeahanzi file not found at {makemeahanzi_path}")
            return {}
        
        radical_decompositions = {}
        
        try:
            with open(makemeahanzi_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        entry = json.loads(line)
                        character = entry.get("character")
                        decomposition = entry.get("decomposition")
                        
                        if character and decomposition:
                            components = self.parse_makemeahanzi_decomposition(decomposition)
                            if components and len(components) > 1:  # Only include composites
                                radical_decompositions[character] = components
                    
                    except json.JSONDecodeError:
                        continue
                        
        except Exception as e:
            print(f"âŒ Error parsing makemeahanzi data: {e}")
            return {}
        
        print(f"âœ… Parsed {len(radical_decompositions)} radical decompositions from makemeahanzi")
        
        # Clean up temporary files if we downloaded them
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
        
        return radical_decompositions

    def populate_radical_decomposition_mapping(self, conn: sqlite3.Connection, decomposition_data: Dict[str, List[str]], existing_radicals: set) -> None:
        """
        Populate radical_decomposition_mapping table with makemeahanzi decomposition data
        
        Args:
            conn: Database connection
            decomposition_data: Dictionary mapping radicals to component lists
            existing_radicals: Set of radicals that exist in the radical system
        """
        cursor = conn.cursor()
        
        print("ðŸ”§ Populating radical decomposition mapping...")
        
        # Chinese â†’ Japanese radical substitutions
        chinese_to_japanese_substitutions = {
            # Fire and line radicals
            'ç¬': 'âº£',  # Chinese fire dots â†’ Japanese fire radical
            'ä¸¨': 'ï½œ',  # CJK radical stick â†’ Japanese fullwidth vertical line
            
            # Common stroke/shape radicals
            'ä¸¿': 'ãƒŽ',  # Left-falling stroke â†’ katakana no (similar shape/meaning)
            'ä¹š': 'ä¹™',  # Hook stroke â†’ second/hook radical
            'äº»': 'äºº',  # Person radical variant â†’ person radical
            
            # Note: å»¿ and å„ should decompose naturally:
            # å»¿ â†’ ['å»¾', 'ä¸€'] and å„ â†’ ['å', 'ä¸¨'] â†’ ['å', 'ï½œ']
            # So no direct substitution needed for these
            
            # Add more substitutions as we research them
        }
        
        # Manual corrections for known issues in makemeahanzi data
        manual_corrections = {
            'ä¸·': ['ä¸¶', 'ä¸¶'],  # Two dots - makemeahanzi has corrupted data with 'ï¼Ÿ'
            'è‚‰': ['å†‚', 'äºº', 'äºº'],  # Meat - flattened from å†‚,ä»Œ â†’ å†‚,äºº,äºº
            # Add more corrections here if needed
        }
        
        # Apply manual corrections first
        corrected_count = 0
        for radical, components in manual_corrections.items():
            if radical in existing_radicals:
                valid_components = [comp for comp in components if comp in existing_radicals]
                if valid_components and len(valid_components) > 1:
                    components_str = ",".join(valid_components)
                    component_count = len(valid_components)
                    
                    cursor.execute("""
                        INSERT OR REPLACE INTO radical_decomposition_mapping (radical, components, component_count)
                        VALUES (?, ?, ?)
                    """, (radical, components_str, component_count))
                    
                    corrected_count += 1
                    print(f"    ðŸ”§ Manual correction: {radical} â†’ {valid_components}")
        
        def apply_substitutions_and_expansion(components):
            """Apply Chineseâ†’Japanese substitutions and recursive expansion"""
            expanded_components = []
            
            for comp in components:
                # Apply Chinese â†’ Japanese substitutions first
                if comp in chinese_to_japanese_substitutions:
                    substitute = chinese_to_japanese_substitutions[comp]
                    expanded_components.append(substitute)
                    print(f"      ðŸ”„ Substituted: {comp} â†’ {substitute}")
                # Recursive expansion for missing components that have decompositions
                elif comp not in existing_radicals and comp in decomposition_data:
                    # Recursively expand missing component
                    sub_components = decomposition_data[comp]
                    # Apply substitutions recursively (but avoid infinite loops)
                    if comp != 'ä»Œ':  # Prevent infinite recursion for ice radical
                        recursive_expansion = apply_substitutions_and_expansion(sub_components)
                        expanded_components.extend(recursive_expansion)
                        print(f"      ðŸ”„ Expanded: {comp} â†’ {recursive_expansion}")
                    else:
                        # Special case for ice radical: ä»Œ â†’ ['äºº', 'äºº']
                        expanded_components.extend(['äºº', 'äºº'])
                        print(f"      ðŸ”„ Expanded ice: {comp} â†’ ['äºº', 'äºº']")
                else:
                    # Keep component as-is
                    expanded_components.append(comp)
            
            return expanded_components
        
        valid_decompositions = 0
        substitution_count = 0
        for radical, components in decomposition_data.items():
            # Skip if we already applied a manual correction
            if radical in manual_corrections:
                continue
                
            # Only include decompositions where the radical exists in our radical system
            if radical in existing_radicals:
                # Apply substitutions and recursive expansion
                expanded_components = apply_substitutions_and_expansion(components)
                
                # Filter to only include components that exist as radicals after expansion
                valid_components = [comp for comp in expanded_components if comp in existing_radicals]
                
                # Allow single components for important hierarchical radicals, otherwise require 2+
                important_single_decompositions = {'å†‚', 'âº£'}  # Box and fire radical create useful hierarchical paths
                min_components = 1 if radical in important_single_decompositions else 2
                
                if valid_components and len(valid_components) >= min_components:
                    components_str = ",".join(valid_components)
                    component_count = len(valid_components)
                    
                    cursor.execute("""
                        INSERT OR REPLACE INTO radical_decomposition_mapping (radical, components, component_count)
                        VALUES (?, ?, ?)
                    """, (radical, components_str, component_count))
                    
                    valid_decompositions += 1
                    
                    # Track substitutions made
                    if len(expanded_components) != len(components):
                        substitution_count += 1
                    
                    # Debug specific examples
                    if radical in ['é­š', 'è‚‰', 'é³¥', 'é¦¬', 'ä¸·'] or len(expanded_components) != len(components):
                        print(f"    ðŸŽ¯ Added decomposition: {radical} â†’ {valid_components} (from {components})")
        
        conn.commit()
        print(f"âœ… Added {valid_decompositions} radical decompositions from makemeahanzi")
        print(f"âœ… Added {corrected_count} manual corrections")
        print(f"ðŸ”„ Applied substitutions/expansions to {substitution_count} radicals")

    def load_kanjidic_data(self, file_path: str) -> List[Dict]:
        """Load KanjiDic data from JSON file"""
        print(f"ðŸ“– Loading KanjiDic data from {file_path}...")

        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Extract characters array from KanjiDic JSON structure
                characters = data.get('characters', [])
                print(f"âœ… Loaded {len(characters)} kanji entries from KanjiDic file")
                return characters
            except Exception as e:
                print(f"âŒ Failed to load KanjiDic data from {file_path}: {e}")
                return []
        else:
            print(f"âš ï¸  KanjiDic file not found at {file_path}")
            print("Please ensure you have kanjidic.json file in the assets directory")
            return []

    def populate_kanji_entries(self, conn: sqlite3.Connection, kanjidic_data: List[Dict], debug_mode: bool = False) -> None:
        """Populate the kanji_entries table"""
        cursor = conn.cursor()

        print(f"ðŸ“ Populating kanji entries... (debug_mode={debug_mode})")
        entries_added = 0

        for i, entry in enumerate(kanjidic_data):
            if i % 50 == 0 and i > 0:
                print(f"   Progress: {i}/{len(kanjidic_data)} kanji...")
                conn.commit()

            # KanjiDic2 format uses 'literal' for the kanji character
            kanji = entry.get('literal', entry.get('kanji', entry.get('character', '')))
            if not kanji:
                continue

            # Extract meanings from KanjiDic2 structure
            meanings = []
            reading_meaning = entry.get('readingMeaning', {})
            if 'groups' in reading_meaning:
                for group in reading_meaning['groups']:
                    if 'meanings' in group:
                        for meaning in group['meanings']:
                            if meaning.get('lang') == 'en':
                                meanings.append(meaning['value'])

            if not meanings and 'meanings' in entry: # Fallback for older formats
                meanings = entry['meanings']

            meanings_json = json.dumps(meanings, ensure_ascii=False)

            # Extract readings from KanjiDic2 structure
            kun_readings = []
            on_readings = []
            nanori_readings = []

            if 'groups' in reading_meaning:
                for group in reading_meaning['groups']:
                    if 'readings' in group:
                        for reading in group['readings']:
                            reading_type = reading.get('type', '')
                            reading_value = reading.get('value', '')

                            if reading_type == 'ja_kun':
                                kun_readings.append(reading_value)
                            elif reading_type == 'ja_on':
                                on_readings.append(reading_value)

            # Get nanori readings
            if 'nanori' in reading_meaning:
                nanori_readings = reading_meaning['nanori']

            # Fallback to old format (if 'groups' not used or empty)
            if not kun_readings and entry.get('kun_readings'):
                kun_readings = entry['kun_readings']
            if not on_readings and entry.get('on_readings'):
                on_readings = entry['on_readings']
            if not nanori_readings and entry.get('nanori_readings'):
                nanori_readings = entry['nanori_readings']

            kun_json = json.dumps(kun_readings if isinstance(kun_readings, list) else [kun_readings], ensure_ascii=False)
            on_json = json.dumps(on_readings if isinstance(on_readings, list) else [on_readings], ensure_ascii=False)
            nanori_json = json.dumps(nanori_readings if isinstance(nanori_readings, list) else [nanori_readings], ensure_ascii=False)

            # Extract metadata from KanjiDic2 structure
            misc = entry.get('misc', {})
            jlpt_level = misc.get('jlptLevel', entry.get('jlpt_level', entry.get('jlpt')))
            grade = misc.get('grade', entry.get('grade', entry.get('grade_level')))
            stroke_counts = misc.get('strokeCounts', [])
            stroke_count = stroke_counts[0] if stroke_counts else entry.get('stroke_count', entry.get('strokes'))
            frequency = misc.get('frequency', entry.get('frequency', entry.get('freq')))

            heisig_number = entry.get('heisig_number', entry.get('heisig'))
            heisig_keyword = entry.get('heisig_keyword', '')

            # Extract radical information from the radicals array
            radicals_list = entry.get('radicals', [])
            radical_number = None
            radical = ''
            
            # Find the classical radical
            for rad in radicals_list:
                if rad.get('type') == 'classical':
                    radical_number = rad.get('value')
                    break
            
            # Get radical name using a lookup table
            radical_names = []
            if radical_number:
                radical_name = get_radical_name(radical_number)
                if radical_name:
                    radical_names = [radical_name]
                    radical = radical_name
            
            radical_names_json = json.dumps(radical_names, ensure_ascii=False)

            components = entry.get('components', [])
            components_json = json.dumps(components if isinstance(components, list) else [components], ensure_ascii=False)

            try:
                cursor.execute("""
                    INSERT OR REPLACE INTO kanji_entries
                    (kanji, jlpt_level, grade, stroke_count, frequency,
                     meanings, kun_readings, on_readings, nanori_readings,
                     radical_names, heisig_number, heisig_keyword,
                     components, radical, radical_number)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (kanji, jlpt_level, grade, stroke_count, frequency,
                      meanings_json, kun_json, on_json, nanori_json,
                      radical_names_json, heisig_number, heisig_keyword,
                      components_json, radical, radical_number))

                entries_added += 1
                
                # In debug mode, commit after every successful insertion
                if debug_mode:
                    conn.commit()
                    if entries_added % 100 == 0:
                        print(f"   ðŸ› Debug: Successfully added {entries_added} kanji so far")
                        
            except sqlite3.Error as e:
                print(f"   âŒ Error inserting kanji '{kanji}': {e}")
                # Only suppress UNIQUE constraint errors, not other issues
                if "UNIQUE constraint failed" not in str(e):
                    print(f"   ðŸ” Non-unique error for '{kanji}': {e}")
                    # Try to commit what we have so far to avoid losing progress
                    try:
                        conn.commit()
                        print(f"   ðŸ’¾ Committed {entries_added} entries so far due to error")
                    except:
                        pass
                continue

        # Final commit
        conn.commit()
        print(f"âœ… Added {entries_added} kanji entries total")

    def load_jmnedict_data(self, file_path: str) -> List[Dict]:
        """Load JMnedict data from JSON file"""
        print(f"ðŸ“› Loading JMnedict data from {file_path}...")

        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Handle new JMnedict format with metadata and words array
                if isinstance(data, dict) and 'words' in data:
                    words = data['words']
                    print(f"âœ… Loaded {len(words)} entries from JMnedict file (new format)")
                    return words
                elif isinstance(data, list):
                    # Old format - direct array of entries
                    print(f"âœ… Loaded {len(data)} entries from JMnedict file (old format)")
                    return data
                else:
                    print(f"âŒ Unexpected JMnedict format")
                    return []
            except Exception as e:
                print(f"âŒ Failed to load JMnedict data from {file_path}: {e}")
                return []
        else:
            print(f"âš ï¸  JMnedict file not found at {file_path}")
            print("Please ensure you have jmnedict.json file in the assets directory")
            return []


    def rebuild_kanji_only(self) -> bool:
        """Rebuild only the kanji_entries table in existing database"""
        print(f"ðŸ”„ Rebuilding kanji data in: {self.output_path}")
        
        if not os.path.exists(self.output_path):
            print(f"âŒ Database not found: {self.output_path}")
            return False
        
        try:
            conn = sqlite3.connect(self.output_path)
            
            # Clear existing kanji data
            print("ðŸ—‘ï¸  Clearing existing kanji data...")
            conn.execute("DELETE FROM kanji_entries")
            conn.commit()
            
            # Load KanjiDic data from single file
            print("ðŸ“– Loading KanjiDic data...")
            kanjidic_file = "app/src/main/assets/kanjidic.json"
            kanjidic_data = self.load_kanjidic_data(kanjidic_file)
            
            if not kanjidic_data:
                print("âŒ No KanjiDic data found!")
                conn.close()
                return False
            
            # Populate kanji entries
            print(f"ðŸ“ Populating {len(kanjidic_data)} kanji entries...")
            self.populate_kanji_entries(conn, kanjidic_data)
            
            # Commit and close
            conn.commit()
            conn.close()
            
            print("âœ… Kanji rebuild completed successfully!")
            return True
            
        except Exception as e:
            print(f"âŒ Kanji rebuild failed: {e}")
            return False

    def build_database(self, jmdict_path: str = "app/src/main/assets/jmdict.json", kanjidic_path: str = "app/src/main/assets/kanjidic.json", jmnedict_path: str = "app/src/main/assets/jmnedict.json", kradfile_path: str = "app/src/main/assets/kradfile.json", radkfile_path: str = "app/src/main/assets/radkfile.json") -> bool:
        """Main method to build the complete database"""
        print(f"ðŸš€ Building database: {self.output_path}")
        print(f"ðŸ“… Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # Remove existing database
        if os.path.exists(self.output_path):
            print("ðŸ—‘ï¸  Removing existing database...")
            os.remove(self.output_path)

        try:
            # Create new database
            conn = sqlite3.connect(self.output_path)
            conn.execute("PRAGMA journal_mode = WAL;") # Enable WAL for better concurrency
            conn.execute("PRAGMA synchronous = NORMAL;") # Optimize write performance

            # Create schema
            print("ðŸ“‹ Creating database schema...")
            self.create_database_schema(conn)

            # Populate tag definitions (JMdict POS tags and JMnedict types)
            print("ðŸ·ï¸  Populating tag definitions...")
            self.populate_tag_definitions(conn)

            # Populate pitch accent data
            print("ðŸŽµ Populating pitch accent data...")
            self.populate_pitch_accents(conn)

            # Process KANJI and RADICAL data FIRST (faster to test)
            print("\n" + "="*50)
            print("ðŸ”§ PROCESSING KANJI & RADICAL DATA (FAST)")
            print("="*50)
            
            # Load and populate kradfile data (kanji â†’ components)
            print("\nðŸ”„ Processing kradfile data...")
            kradfile_data = self.load_kradfile_data(kradfile_path)
            if kradfile_data:
                print(f"ðŸ“Š Loaded {len(kradfile_data)} kanji entries from kradfile")
                self.populate_kanji_radical_mapping(conn, kradfile_data)
            else:
                print("âš ï¸  No kradfile data found, skipping kanji radical mapping.")
            
            # Load radkfile data (for stroke counts and additional radicals)
            print("\nðŸ”„ Loading radkfile data...")
            radkfile_data = self.load_radkfile_data(radkfile_path)
            if radkfile_data:
                print(f"ðŸ“Š Loaded {len(radkfile_data)} radicals from radkfile")
                
                # Show sample radical data for debugging
                sample_radicals = list(radkfile_data.keys())[:5]
                for radical in sample_radicals:
                    kanji_count = len(radkfile_data[radical].get('kanji', []))
                    stroke_count = radkfile_data[radical].get('strokeCount', 0)
                    print(f"  ðŸ” {radical}: {kanji_count} kanji, {stroke_count} strokes")
                
                # Check specifically for äºº radical
                if 'äºº' in radkfile_data:
                    person_kanji = radkfile_data['äºº']['kanji']
                    has_fire = 'ç«' in person_kanji
                    print(f"  ðŸŽ¯ äºº radical: {len(person_kanji)} kanji, contains ç«: {has_fire}")
                else:
                    print("  âŒ äºº radical not found in radkfile!")
            else:
                print("âš ï¸  No radkfile data found!")

            # Build radical â†’ kanji mapping from kradfile (primary) with radkfile support
            print("\nðŸ”§ Building radical â†’ kanji mapping...")
            if kradfile_data:
                self.populate_radical_kanji_mapping_from_kradfile(conn, kradfile_data, radkfile_data)
                
                # Verify the database entries
                print("\nðŸ” Verifying radical database entries...")
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM radical_kanji_mapping")
                radical_count = cursor.fetchone()[0]
                print(f"  ðŸ“Š Total radicals in database: {radical_count}")
                
                # Check äºº radical specifically
                cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = 'äºº'")
                person_result = cursor.fetchone()
                if person_result:
                    radical, strokes, kanji_list = person_result
                    kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                    has_fire = 'ç«' in kanji_list if kanji_list else False
                    print(f"  ðŸŽ¯ Database äºº radical: {kanji_count} kanji, {strokes} strokes, contains ç«: {has_fire}")
                    if has_fire:
                        print("    âœ… SUCCESS: ç« found in äºº radical!")
                    else:
                        print("    âŒ ISSUE: ç« NOT found in äºº radical")
                else:
                    print("  âŒ äºº radical not found in database!")
                    
            elif radkfile_data:
                # Fallback to old method if only radkfile is available
                self.populate_radical_kanji_mapping(conn, radkfile_data)
            else:
                print("âš ï¸  No radical data found, skipping radical kanji mapping.")
            
            # POST-PROCESSING: Override ALL radicals with enhanced radkfile data
            if radkfile_data:
                print("\nðŸ”§ POST-PROCESSING: Enhancing ALL radicals with radkfile data...")
                cursor = conn.cursor()
                
                updated_count = 0
                for radical, info in radkfile_data.items():
                    enhanced_kanji_list = info.get('kanji', [])
                    if enhanced_kanji_list:  # Only update if there's data
                        kanji_str = ", ".join(enhanced_kanji_list)
                        
                        # Debug äºº radical specifically
                        if radical == 'äºº':
                            print(f"  ðŸ” Updating äºº radical with {len(enhanced_kanji_list)} kanji")
                            print(f"  ðŸ” ç« in list: {'ç«' in enhanced_kanji_list}")
                            print(f"  ðŸ” First 10 kanji: {enhanced_kanji_list[:10]}")
                        
                        cursor.execute("""
                            UPDATE radical_kanji_mapping 
                            SET kanji_list = ?
                            WHERE radical = ?
                        """, (kanji_str, radical))
                        
                        if cursor.rowcount > 0:
                            updated_count += 1
                            if radical == 'äºº':
                                print(f"  âœ… Successfully updated äºº radical (rowcount={cursor.rowcount})")
                
                conn.commit()
                print(f"  âœ… Updated {updated_count} radicals with enhanced data")
                
                # Verify äºº radical specifically
                cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = 'äºº'")
                result = cursor.fetchone()
                if result:
                    radical, strokes, kanji_list = result
                    kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                    has_fire = 'ç«' in kanji_list if kanji_list else False
                    print(f"  ðŸŽ¯ äºº radical after update: {kanji_count} kanji, contains ç«: {has_fire}")
                    if has_fire:
                        print("    âœ… SUCCESS: ç« found in äºº radical!")
                
                print("  âœ… Post-processing complete!")
                
                # Final verification before moving on
                print("\nðŸ” FINAL VERIFICATION of radical data...")
                cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = 'äºº'")
                final_result = cursor.fetchone()
                if final_result:
                    radical, strokes, kanji_list = final_result
                    kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                    has_fire = 'ç«' in kanji_list if kanji_list else False
                    print(f"  ðŸŽ¯ FINAL CHECK - äºº radical: {kanji_count} kanji, contains ç«: {has_fire}")
                    
                    # Also check a few specific kanji
                    if kanji_list:
                        kanji_array = kanji_list.split(', ')
                        print(f"  ðŸ“ First 5 kanji: {kanji_array[:5]}")
                        if 'ç«' in kanji_array:
                            fire_index = kanji_array.index('ç«')
                            print(f"  ðŸ”¥ ç« is at position {fire_index + 1}")
                else:
                    print("  âŒ äºº radical not found in final check!")

            # RADICAL DECOMPOSITION: Load and populate makemeahanzi decomposition data
            print("\nðŸ§© Processing radical decomposition data...")
            decomposition_data = self.load_makemeahanzi_decomposition_data()
            if decomposition_data:
                # Get existing radicals from the database to filter valid decompositions
                cursor.execute("SELECT radical FROM radical_kanji_mapping")
                existing_radicals = {row[0] for row in cursor.fetchall()}
                print(f"ðŸ“Š Found {len(existing_radicals)} existing radicals in database")
                
                self.populate_radical_decomposition_mapping(conn, decomposition_data, existing_radicals)
                
                # Verify specific decomposition example
                cursor.execute("SELECT radical, components FROM radical_decomposition_mapping WHERE radical = 'ä¸·'")
                result = cursor.fetchone()
                if result:
                    radical, components = result
                    print(f"  ðŸŽ¯ Example decomposition: {radical} â†’ {components}")
                else:
                    print("  âš ï¸  No decomposition example found for ä¸·")
            else:
                print("âš ï¸  No makemeahanzi decomposition data found, skipping radical decomposition.")

            # Load and populate KanjiDic data
            print("\nðŸ“– Processing KanjiDic data...")
            kanjidic_data = self.load_kanjidic_data(kanjidic_path)
            if kanjidic_data:
                print(f"ðŸ“Š Loaded {len(kanjidic_data)} kanji from KanjiDic")
                self.populate_kanji_entries(conn, kanjidic_data)
            else:
                print("âš ï¸  No KanjiDic data found, skipping KanjiDic entries.")

            print("\n" + "="*50)
            print("ðŸŽ‰ KANJI & RADICAL DATA COMPLETE!")
            print("You can test the radical search now or continue with full build...")
            print("="*50)
            
            # Process DICTIONARY data LAST (slower, for complete build)
            print("\nðŸ“– Processing JMdict data (this will take time)...")
            
            # Load JMdict data from single file
            jmdict_data = self.load_jmdict_data(jmdict_path)
            if not jmdict_data:
                print("ðŸš¨ No JMdict data loaded. Cannot build dictionary.")
                return False

            print(f"ðŸ“Š Loaded {len(jmdict_data)} entries from JMdict")

            # Load JMDict specific tags data
            tags_file_path = os.path.join(os.path.dirname(jmdict_path), "tags.json")
            tags_data_jmdict = self.load_tags_data(tags_file_path)

            # Populate dictionary data with JMdict entries
            print("ðŸ”„ Populating JMdict entries...")
            self.populate_dictionary_entries(conn, jmdict_data, tags_data_jmdict, is_jmnedict_source=False)

            # Load and populate JMnedict data (names dictionary) from single file
            print("\nðŸ“› Processing JMnedict data...")
            jmnedict_data = self.load_jmnedict_data(jmnedict_path)
            if jmnedict_data:
                print(f"ðŸ“Š Loaded {len(jmnedict_data)} entries from JMnedict")
                # JMnedict tags are embedded in the main file, pass empty dict for tags_data
                print(f"ðŸ“› Adding {len(jmnedict_data)} JMnedict entries to dictionary tables...")
                # Pass True for is_jmnedict_source for all JMnedict entries
                self.populate_dictionary_entries(conn, jmnedict_data, {}, is_jmnedict_source=True)
            else:
                print("âš ï¸  No JMnedict data found, skipping name entries.")

            # Load and populate KanjiDic data from single file
            print("\nðŸˆµ Processing KanjiDic data...")
            kanjidic_data = self.load_kanjidic_data(kanjidic_path)
            if kanjidic_data:
                self.populate_kanji_entries(conn, kanjidic_data)
            else:
                print("âš ï¸  No KanjiDic data found, skipping kanji tables.")

            # NOTE: kradfile and radkfile processing already done at the beginning with post-processing

            # Populate FTS tables (after all data is inserted into main table)
            self.populate_fts_tables(conn)

            # Create triggers (after FTS tables are created)
            self.create_triggers(conn)

            # Verify database
            if not self.verify_database(conn):
                return False

            # Check äºº radical before optimization
            print("\nðŸ” Checking äºº radical BEFORE optimization...")
            cursor = conn.cursor()
            cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = 'äºº'")
            before_result = cursor.fetchone()
            if before_result:
                radical, strokes, kanji_list = before_result
                kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                has_fire = 'ç«' in kanji_list if kanji_list else False
                print(f"  ðŸŽ¯ BEFORE VACUUM - äºº radical: {kanji_count} kanji, contains ç«: {has_fire}")
            
            # Optimize database
            print("\nðŸ”§ Optimizing database...")
            conn.execute("VACUUM")
            conn.execute("ANALYZE")
            
            # Check äºº radical after optimization
            print("\nðŸ” Checking äºº radical AFTER optimization...")
            cursor.execute("SELECT radical, stroke_count, kanji_list FROM radical_kanji_mapping WHERE radical = 'äºº'")
            after_result = cursor.fetchone()
            if after_result:
                radical, strokes, kanji_list = after_result
                kanji_count = len(kanji_list.split(', ')) if kanji_list else 0
                has_fire = 'ç«' in kanji_list if kanji_list else False
                print(f"  ðŸŽ¯ AFTER VACUUM - äºº radical: {kanji_count} kanji, contains ç«: {has_fire}")

            conn.close()

            # Get final size
            size_mb = os.path.getsize(self.output_path) / (1024 * 1024)
            print(f"ðŸ“Š Final database size: {size_mb:.2f} MB")
            print(f"âœ… Database built successfully: {self.output_path}")
            print(f"ðŸ“… Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

            return True

        except Exception as e:
            print(f"âŒ Database build failed: {e}")
            # print stack trace for debugging
            import traceback
            traceback.print_exc()
            return False

def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Build FTS5-enabled dictionary database")
    parser.add_argument("--jmdict", default="app/src/main/assets/jmdict.json",
                       help="Path to JMdict JSON file")
    parser.add_argument("--output", default="app/src/main/assets/databases/jmdict_fts5.db",
                       help="Output database path")
    parser.add_argument("--kanjidic", default="app/src/main/assets/kanjidic.json",
                       help="Path to KanjiDic JSON file")
    parser.add_argument("--jmnedict", default="app/src/main/assets/jmnedict.json",
                       help="Path to JMnedict JSON file")
    parser.add_argument("--kradfile", default="app/src/main/assets/kradfile.json",
                       help="Path to kradfile JSON file")
    parser.add_argument("--radkfile", default="app/src/main/assets/radkfile.json",
                       help="Path to radkfile JSON file")

    args = parser.parse_args()

    builder = DatabaseBuilder(args.output)
    success = builder.build_database(args.jmdict, args.kanjidic, args.jmnedict, args.kradfile, args.radkfile)

    if success:
        print("\nðŸŽ‰ Database ready for Android deployment!")
        print(f"   Location: {args.output}")
        print("   Contains: JMdict entries + JMnedict names + KanjiDic kanji data")
        print("   The app will automatically detect and use the pre-built database.")
    else:
        print("\nâŒ Database build failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()